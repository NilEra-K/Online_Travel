# Online_Travel
OnlineTravelBigdataPlatform

## ğŸŒ³ åœ¨çº¿æ—…æ¸¸å¤§æ•°æ®å¹³å°é¡¹ç›®
<p>
    <label for="file">å®Œæˆåº¦ï¼š</label>
    <progress max="100" value="100"> 100% </progress>
</p>


**æ—¥æœŸä»»åŠ¡æ¸…å•**

- [x] é¡¹ç›®å¼€å§‹æ—¥æœŸ: 2024-06-11
  - [x] äº†è§£é¡¹ç›®çš„èƒŒæ™¯ä»¥åŠæ•´ä¸ªç³»ç»Ÿçš„æ¶æ„
  - [x] äº†è§£ç³»ç»Ÿéœ€è¦å®Œæˆçš„ä¸»è¦åŠŸèƒ½
  - [x] äº†è§£ç³»ç»Ÿæ•´ä¸ªæ¶æ„
  - [x] å®Œæˆæ•°æ®æœåŠ¡ç«¯çš„éƒ¨ç½²
  - [x] å®Œæˆæ•°æ®å®¢æˆ·ç«¯çš„éƒ¨ç½²
  - [x] äº†è§£æ•°æ®é›†
  - [x] è®¤è¯†æ¶ˆæ¯é˜Ÿåˆ— Kafka
  - [x] å®Œæˆæ¶ˆæ¯é˜Ÿåˆ— Kafka çš„éƒ¨ç½²
  - [x] äº†è§£æ¶ˆæ¯é˜Ÿåˆ— Kafka çš„åŸºæœ¬åº”ç”¨
  - [x] ä½¿ç”¨ Flume æ”¶é›†æ•°æ®åˆ° Kafka
- [x] 2024-06-12
  - [x] äº†è§£å®æ—¶æ•°æ®åˆ†ææ‰€ç”¨åˆ°çš„æŠ€æœ¯
  - [x] äº†è§£ `SparkStreaming` å’Œ `Flink`
  - [x] äº†è§£ `SparkStreaming` çš„æ ¸å¿ƒæ¦‚å¿µ
  - [x] äº†è§£æ•°æ®æº
  - [x] å€ŸåŠ©`netcat`å®è·µ`Kafka`
  - [x] äº†è§£è½¬æ¢æ“ä½œ
  - [x] å…·ä½“å®æ–½ä»»åŠ¡ï¼šå¤„ç†æ•°æ®

- [x] 2024-06-13 
  - [x] äº†è§£æ•°æ®åº“è¿æ¥æ± 
  - [x] äº†è§£å¦‚ä½•å‘ MySQL æ•°æ®åº“ä¸­å†™å…¥æ•°æ®
  - [x] å€ŸåŠ© `alibaba Druid` åº“å®ç°ä¸€ä¸ªæ•°æ®åº“è¿æ¥å·¥å…·ç±» 
  - [x] ç¼–å†™æ¡ˆä¾‹ï¼š`WordCount`
  - [x] å…·ä½“å®æ–½ä»»åŠ¡ï¼Œå°†ä»£ç ä¸­ç”Ÿæˆçš„æ•°æ®å†™å…¥`MySQL`ã€‚

- [x] 2024-06-14
  - [x] äº†è§£ä»€ä¹ˆæ˜¯ Kafka Offset
  - [x] ç»´æŠ¤ Kafka Offset
  - [x] å…·ä½“å®æ–½ä»»åŠ¡ï¼Œå°† Kafka çš„ Offset é…åˆ `MySQL` ç”¨ä»£ç è¿›è¡Œç»´æŠ¤ã€‚

- [x] 2024-06-17
  - [x] è¿›è¡Œåç«¯å¼€å‘
  - [x] è¿›è¡Œå‰ç«¯å¼€å‘

- [x] 2024-06-18
  - [x] è¿›è¡Œçƒ­åŠ›å›¾çš„ç»˜åˆ¶
  - [x] è¿›è¡Œäººæµé‡æŸ±çŠ¶å›¾çš„ç»˜åˆ¶
  - [x] è¿›è¡Œäººæµé‡è¶‹åŠ¿å›¾çš„ç»˜åˆ¶

- [x] 2024-06-19 é¡¹ç›®ç»“æŸæ—¥æœŸ

**æ–‡ä»¶ç›®å½•**

[TOC]

<!-- more -->

### ğŸ“• 1. é¡¹ç›®æ¦‚è¿°
#### 1.1 é¡¹ç›®èƒŒæ™¯
éšç€ä¿¡æ¯æŠ€æœ¯çš„é£é€Ÿå‘å±•ï¼Œæ—…æ¸¸è¡Œä¸šæ­£è¿…é€Ÿèå…¥æ•°å­—åŒ–è½¬å‹çš„æµªæ½®ä¸­ã€‚æ—…æ¸¸å¤§æ•°æ®çš„äº§ç”Ÿå’Œç§¯ç´¯ä¸ºè¡Œä¸šæä¾›äº†å‰æ‰€æœªæœ‰çš„æ´å¯ŸåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ•°æ®å¤„ç†æ–¹æ³•å¾€å¾€éš¾ä»¥åº”å¯¹æ•°æ®çš„æµ·é‡æ€§å’Œå®æ—¶æ€§éœ€æ±‚ã€‚
#### 1.2 é¡¹ç›®ä»‹ç»
æœ¬é¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ªæ—…æ¸¸å¤§æ•°æ®å®æ—¶åˆ†æå’Œç›‘æ§ç³»ç»Ÿï¼Œç³»ç»Ÿä¸»è¦åŒ…æ‹¬æ—…æ¸¸æ•°æ®åˆ†æå’Œå®æ—¶ç›‘æ§ä¸¤å¤§æ¨¡å—ï¼Œæ—…æ¸¸æ•°æ®åˆ†ææ¨¡å—æ˜¯åŸºäºSpark Streamingå¯¹æµå—å„æ™¯ç‚¹çš„äººæµé‡æ•°æ®è¿›è¡Œå®æ—¶å¤„ç†å’Œåˆ†æï¼Œå®æ—¶ç›‘æ§æ¨¡å—æ˜¯åŸºäºSpringBootå¯¹åˆ†æçš„ç»“æœè¿›è¡Œå¯è§†åŒ–å±•ç¤ºã€‚ 

### ğŸš§ 2. ç³»ç»ŸåŠŸèƒ½åŠæ¶æ„
#### 2.1 ç³»ç»Ÿä¸»è¦åŠŸèƒ½
- æ•°æ®å®æ—¶æ”¶é›†ï¼šé€šè¿‡ ***Flume*** å®æ—¶é‡‡é›†æ‰‹æœºç§»åŠ¨ä¿¡ä»¤æ•°æ®ï¼ˆæ•°æ®ç”Ÿæˆå™¨ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•°æ®ï¼‰ï¼Œå‘é€åˆ° **Kafka**ã€‚
- æ•°æ®å®æ—¶å¤„ç†åˆ†æï¼šé€šè¿‡***Spark Streaming*** æ¶ˆè´¹ ***Kafka*** æ•°æ®ï¼Œä¸»è¦å®Œæˆä»¥ä¸‹åˆ†æï¼š
  - å„æ™¯ç‚¹äººæµé‡å®æ—¶ç»Ÿè®¡ï¼ˆçƒ­åŠ›å›¾ï¼Œæ¯ç§’é’Ÿï¼‰
  - å„æ™¯ç‚¹äººæµé‡éšæ—¶é—´å¢é•¿æƒ…å†µ/å„æ™¯ç‚¹äººæµé‡éšæ—¶é—´å˜åŒ–è¶‹åŠ¿(æ¯åˆ†é’Ÿ)
  - å®æ—¶ç›‘æ§ï¼šé€šè¿‡ `SpringBoot` + `MyBatis` æ„å»ºæ—…æ¸¸ç›‘æ§ç³»ç»Ÿï¼ŒåŸºäºé«˜å¾·åœ°å›¾å®Œæˆæ¯ç§’é’Ÿäººæµé‡çƒ­åŠ›å›¾å±•ç¤ºï¼ŒåŸºäº `Echarts` å®Œæˆæ¯åˆ†é’Ÿæµé‡æŸ±çŠ¶å›¾å’Œæ¯åˆ†é’Ÿäººæµé‡å˜åŒ–æŠ˜çº¿å›¾ã€‚


#### 2.2 ç³»ç»Ÿç»“æ„ä¸æŠ€æœ¯é€‰å‹

<img src="https://img-blog.csdnimg.cn/direct/0b6829b9886840a382e4d6e5a18d366b.png#pic_center" alt="SystemStructure" style="zoom: 50%;" />

- **é¡¹ç›®å¼€å‘å·¥å…·ï¼š**`IntelliJ IDEA 2019`

- **æ•°æ®æ”¶é›†åˆ†æ**ï¼š`Flume` + `Kafka` + `SparkStreaming` + `MySQL/Redis`

- **æ•°æ®å±•ç¤ºï¼š**`SpringBoot` + `MyBatis` + `WebSocket` + `MySQL` + `LayUI` + `Echarts` + `é«˜å¾·åœ°å›¾API`

### ğŸ”§ 3.é¡¹ç›®æ”¶é›†åŠŸèƒ½
#### 3.1 æ•°æ®æœåŠ¡ç«¯ä¸æ•°æ®å®¢æˆ·ç«¯éƒ¨ç½²

æˆ‘çš„ä¸»æœºä¿¡æ¯å¦‚ä¸‹ï¼š

```
192.168.26.110      bigdata
192.168.26.111      webserver01
192.168.26.111      webserver02
```

##### 3.1.1 æ•°æ®æœåŠ¡ç«¯éƒ¨ç½²

1. å°† `logweb-1.0.jar` ä¸Šä¼ åˆ°æœåŠ¡å™¨`webserver01` ä»¥åŠ `webserver02`ã€‚

2. å¯åŠ¨è¿è¡Œ `logweb` ç¨‹åº

   ```bash
   nohup java -jar logweb-1.0.jar &
   ```

   è¿è¡ŒæˆåŠŸåï¼ŒæŸ¥è¯¢æ—¥å¿—æ–‡ä»¶ç»“æœï¼Œç»“æœå¦‚ä¸‹è¡¨ç¤ºæ­£å¸¸å¯åŠ¨ï¼š
   ```
     .   ____          _            __ _ _
    /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
   ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
    \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
     '  |____| .__|_| |_|_| |_\__, | / / / /
    =========|_|==============|___/=/_/_/_/
    :: Spring Boot ::               (v2.7.10)
   
   2024-06-11 12:10:18.084  INFO 1288 --- [           main] c.s.i.w.s.web.logweb.LogwebApplication   : Starting LogwebApplication v1.0 using Java 1.8.0_231 on webserver01 with PID 1288 (/home/subowen/serverJar/logweb-1.0.jar started by subowen in /home/subowen/serverJar)
   2024-06-11 12:10:18.090  INFO 1288 --- [           main] c.s.i.w.s.web.logweb.LogwebApplication   : No active profile set, falling back to 1 default profile: "default"
   2024-06-11 12:10:20.604  INFO 1288 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 9527 (http)
   2024-06-11 12:10:20.666  INFO 1288 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
   2024-06-11 12:10:20.666  INFO 1288 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.73]
   2024-06-11 12:10:21.301  INFO 1288 --- [           main] o.a.c.c.C.[.[localhost].[/logweb]        : Initializing Spring embedded WebApplicationContext
   2024-06-11 12:10:21.301  INFO 1288 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 3094 ms
   2024-06-11 12:10:23.099  INFO 1288 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 9527 (http) with context path '/logweb'
   2024-06-11 12:10:23.116  INFO 1288 --- [           main] c.s.i.w.s.web.logweb.LogwebApplication   : Started LogwebApplication in 6.507 seconds (JVM running for 8.07)
   2024-06-11 12:14:14.608  INFO 1288 --- [nio-9527-exec-1] o.a.c.c.C.[.[localhost].[/logweb]        : Initializing Spring DispatcherServlet 'dispatcherServlet'
   2024-06-11 12:14:14.608  INFO 1288 --- [nio-9527-exec-1] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'
   2024-06-11 12:14:14.609  INFO 1288 --- [nio-9527-exec-1] o.s.web.servlet.DispatcherServlet        : Completed initialization in 1 ms
   ```

##### 3.1.2 æ•°æ®å®¢æˆ·ç«¯éƒ¨ç½²

æœ¬æ¬¡æ•°æ®å®¢æˆ·ç«¯ç›´æ¥éƒ¨ç½²åœ¨ **`Windosw`** ä¸‹ï¼Œä½¿ç”¨ `IntelliJ IDEA 2019` è¿›è¡Œå¼€å‘ã€‚

`IDEA` æ•°æ®å®¢æˆ·ç«¯ç»“æ„å¦‚ä¸‹ï¼š

<img src="https://img-blog.csdnimg.cn/direct/b7a696fb3de3430c843302479bd8f4ff.png#pic_center" alt="LogClientStruct" style="zoom: 80%;" />

1. ä¿®æ”¹ `pom.xml` å¼•å…¥ç›¸åº”çš„åŒ…

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>travel_subowen423</artifactId>
        <groupId>com.example.x.travel</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>logclient</artifactId>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <!-- é…ç½® Spark çš„ç‰ˆæœ¬ -->
        <spark.version>3.0.1</spark.version>
        <httpclient.version>4.5.12</httpclient.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-lang3</artifactId>
            <version>3.12.0</version>
        </dependency>

        <dependency>
            <groupId>org.apache.httpcomponents</groupId>
            <artifactId>httpclient</artifactId>
            <version>${httpclient.version}</version>
        </dependency>

        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>1.2.17</version>
        </dependency>
    </dependencies>
</project>
```

2. å¯¼å…¥ `logclient` ä»£ç ï¼Œå¹¶ä¸”ä¿®æ”¹ç›¸åº”åŒ…åå’Œéƒ¨åˆ†ä»£ç 
3. æ‰§è¡Œ `ScenicAPP.java` éšæœºç”Ÿæˆæ•°æ®ã€‚

##### 3.1.3 æ•°æ®æ ¼å¼è¯´æ˜

| ç»åº¦ | çº¬åº¦ | æ™¯ç‚¹åç§° | æ—¶é—´ |
| :--: | :--: | :------: | :--: |
|117.024489|36.669213|æ›²æ°´äº­è¡—|20240611155103|
|117.016089|36.661138|è¶µçªæ³‰|20240611155103|
|116.813744|36.541549|æµå—å›½é™…å›­åšå›­|20240611155103|
|117.022959|36.668068|èŠ™è“‰è¡—|20240611155103|
|117.034920|36.641749|åƒä½›å±±|20240611155103|
|117.023837|36.674997|å¤§æ˜æ¹–|20240611155103|
|117.023837|36.674997|å¤§æ˜æ¹–|20240611155103|
|117.024489|36.669213|æ›²æ°´äº­è¡—|20240611155103|
|117.016089|36.661138|è¶µçªæ³‰|20240611155103|
|117.021483|36.661473|æµå—æ³‰åŸå¹¿åœº|20240611155103|
|...|...|...|...|

#### 3.2 Kafka æ¶ˆæ¯é˜Ÿåˆ—
##### 3.2.1 Kafka ç®€ä»‹

ç‚¹å‡»è®¿é—®ï¼š[Kafkaå®˜ç½‘](https://kafka.apache.org/)

***Apache Kafka*** æ˜¯ä¸€ä¸ªå¼€æºåˆ†å¸ƒå¼**äº‹ä»¶(Event)**æµå¹³å°ï¼Œå·²è¢«æ•°åƒå®¶å…¬å¸ç”¨äºé«˜æ€§èƒ½æ•°æ®ç®¡é“ã€æµåˆ†æã€æ•°æ®é›†æˆå’Œå…³é”®ä»»åŠ¡åº”ç”¨ç¨‹åºã€‚

æœ¬é¡¹ç›®ä¸»è¦ä½¿ç”¨**å‘å¸ƒ**ï¼ˆå†™å…¥ï¼‰å’Œ**è®¢é˜…**ï¼ˆè¯»å–ï¼‰äº‹ä»¶æµï¼ŒåŒ…æ‹¬ä»å…¶ä»–ç³»ç»ŸæŒç»­å¯¼å…¥/å¯¼å‡ºæ•°æ®ï¼›æ ¹æ®éœ€è¦æŒä¹…å¯é åœ°**å­˜å‚¨**äº‹ä»¶æµï¼›åœ¨äº‹ä»¶å‘ç”Ÿæ—¶æˆ–å›é¡¾æ€§åœ°**å¤„ç†**äº‹ä»¶æµã€‚

##### 3.2.2 Kafka ä¸­çš„æ ¸å¿ƒæ¦‚å¿µ

| åè¯ | è§£é‡Š |
| :--- | :--- |
|`Broker`|***Kafka*** é›†ç¾¤åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªæœåŠ¡å™¨ï¼Œè¿™äº›æœåŠ¡å™¨ç§°ä¸º `Broker`|
|`Producer`|ç”Ÿäº§è€…ï¼Œè´Ÿè´£å°†æ•°æ®å‘é€åˆ° ***Kafka***|
|`Consumer`|æ¶ˆè´¹è€…ï¼Œè´Ÿè´£ä» ***Kafka*** ä¸­è¯»å–æ•°æ®|
|`Consumer Group`|æ¶ˆè´¹è€…ç»„ï¼Œå¤šä¸ªæ¶ˆè´¹è€…ç»„æˆçš„ç»„|
|`Topic`|ä¸»é¢˜ï¼Œæ¯æ¡å‘å¸ƒåˆ° ***Kafka*** é›†ç¾¤çš„æ¶ˆæ¯éƒ½æœ‰ä¸€ä¸ªç±»åˆ«ï¼Œè¿™ä¸ªç±»åˆ«ç§°ä¸º `Topic`ï¼Œå¯ä»¥ç†è§£ä¸ºæ–‡ä»¶å¤¹|
|`Partition`|åˆ†åŒºï¼Œæ¯ä¸ª`Topic`åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ª`Partition`|

##### 3.2.3 Kafka éƒ¨ç½²

- **Kafka** çš„éƒ¨ç½²æ–¹å¼åˆ†ä¸ºï¼š
  - åˆ†å¸ƒå¼éƒ¨ç½²ï¼ˆå¤šèŠ‚ç‚¹å¤šBrokerï¼‰
  - å•æœºéƒ¨ç½²ï¼ˆå•èŠ‚ç‚¹å•Broker/å•èŠ‚ç‚¹å¤šBrokerï¼‰

- **Kafka** æ˜¯ä½¿ç”¨ **Scala** ç¼–å†™çš„ç»„ä»¶ï¼Œä¾èµ–ä¸Scalaç‰ˆæœ¬

- **Kafka** ä¾èµ–äº **ZooKeeper**ï¼Œå¿…é¡»è¦å®‰è£… **ZooKeeper**ï¼Œå†å®‰è£… **Kafka**

- éƒ¨ç½² **Kafka** è¿‡ç¨‹

  - è§£å‹ `kafka` å®‰è£…åŒ…
  - é…ç½®ç¯å¢ƒå˜é‡
  - ä¿®æ”¹é…ç½® `config/server.properties` æ–‡ä»¶

  ```properties
  # é…ç½® Broker çš„ ID, åœ¨åŒä¸€ä¸ªé›†ç¾¤ä¸Š, è¿™ä¸ªå€¼å¿…é¡»æ˜¯ä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„æ•´æ•°å€¼
  broker.id=0
  
  # é…ç½®æ—¥å¿—æ–‡ä»¶çš„è·¯å¾„
  log.dirs=/home/subowen/apps/kafka_2.12-2.8.0/kafka-logs
  ```

##### 3.2.4 Kafka çš„åŸºæœ¬åº”ç”¨

- å¯åŠ¨ **ZooKeeper**

  ```bash
  [subowen@bigdata ~]$ zkServer.sh start
  ```

- å¯åŠ¨ `Kafka`

  - ç¬¬ä¸€æ¬¡å¯åŠ¨ï¼šå…ˆä½¿ç”¨å‰å°å¯åŠ¨ï¼Œå¦‚æœæ²¡æœ‰é—®é¢˜å†ä½¿ç”¨åå°å¯åŠ¨

  - å‰å°å¯åŠ¨å‘½ä»¤ï¼š`kafka-server-start.sh  $KAFKA_HOME/config/server.properties`

  - åå°å¯åŠ¨å‘½ä»¤ï¼š`kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties`

- å¸¸ç”¨çš„ `Kafka` å‘½ä»¤

  å‚è€ƒåšå®¢ï¼š[Kafka åŸºç¡€ç†è®ºä¸å¸¸ç”¨å‘½ä»¤è¯¦è§£ (è¶…è¯¦ç»†) Kafkaå¸¸ç”¨å‘½ä»¤å’Œè§£é‡Š - CSDNåšå®¢](https://blog.csdn.net/zcs2312852665/article/details/134979882)

  - `Topics` å¸¸ç”¨å‘½ä»¤

    - åˆ›å»ºä¸€ä¸ªåä¸º`my_topic`çš„ä¸»é¢˜ï¼š

    ```bash
    [subowen@bigdata ~]$ kafka-topics.sh --create --bootstrap-server bigdata:9092 --topic my_topic
    ```

    - æŸ¥çœ‹åä¸º`my_topic`çš„ä¸»é¢˜çš„è¯¦ç»†ä¿¡æ¯ï¼š

    ```bash
    [subowen@bigdata ~]$ kafka-topics.sh --describe --bootstrap-server bigdata:9092 --topic my_topic
    ```

  - `Producer` å¸¸ç”¨å‘½ä»¤

    - ç”Ÿäº§ä¿¡æ¯

    ```bash
    [subowen@bigdata ~]$ kafka-console-producer.sh --broker-list bigdata:9092 --topic my_topic
    ```

  - `Consumer` å¸¸ç”¨å‘½ä»¤


##### 3.2.5 ä½¿ç”¨ Flume æ”¶é›†æ•°æ®åˆ° Kafka
åœ¨ä¹‹å‰çš„é¡¹ç›®ä¸­ï¼Œæˆ‘å­¦ä¹ äº† ***ç¦»çº¿ç±»å‹*** é¡¹ç›®çš„ Flume æ•°æ®æ”¶é›†ï¼Œå› ä¸ºæœ¬æ¬¡éœ€è¦è¿›è¡Œåœ¨çº¿çš„å®æ—¶æ•°æ®åˆ†æï¼Œæ‰€ä»¥æŒ‰ç…§ä¹‹å‰çš„ç¦»çº¿åˆ†ææ–¹å¼ï¼Œä½¿ç”¨ Flume å°†æ•°æ®æ”¶é›†åˆ° HDFS æ˜¯ä¸é€‚åˆå®æ—¶çš„æ•°æ®åˆ†æç¯å¢ƒçš„ã€‚
å°†æ•°æ®è½åœ°åˆ° HDFS åˆ™æ„å‘³ç€æ•°æ®è¿›å…¥ç£ç›˜ï¼Œæ•°æ®çš„è¯»å†™ä¼šå ç”¨å¤§é‡çš„ç£ç›˜ I/Oï¼Œä¸é€‚ç”¨äºå®æ—¶åœºæ™¯ã€‚
å› æ­¤åœ¨å®æ—¶é¡¹ç›®ï¼Œè€ƒè™‘åˆ°æ•°æ®çš„å®æ—¶æ€§ï¼Œæœ¬æ¬¡å®æ—¶æ•°æ®åˆ†æé¡¹ç›®ä½¿ç”¨ æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆKafkaï¼‰è¿›è¡Œæ•°æ®çš„å­˜æ”¾ã€‚

1. é…ç½® `webserver01` å’Œ `webserver02` ä¸Šçš„ Flume é…ç½®æ–‡ä»¶ `taildir-avro-stream.conf`
```conf
a1.sources  = r1
a1.sinks    = k1
a1.channels = c1

a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /home/subowen/apps/apache-flume-1.9.0-bin/position/taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /home/subowen/serverJar/logweb/logs/scenic.log
a1.sources.r1.headers.f1.headerKey1 = inspur-szy
a1.sources.r1.fileHeader = true
a1.sources.r1.maxBatchCount = 1000

a1.sinks.k1.type = avro
a1.sinks.k1.hostname = 192.168.26.110
a1.sinks.k1.port = 4545

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1
```

2. é…ç½® `bigdata` ä¸Šçš„ Flume é…ç½®æ–‡ä»¶ `avro-kafka-stream.conf`
```conf
# é…ç½® Agent a1å„ä¸ªç»„ä»¶çš„åç§°
# Agent a1 çš„ sourceæœ‰ä¸€ä¸ª, å«åšr1
a1.sources  = r1
# Agent a1 çš„ sinkä¹Ÿæœ‰ä¸€ä¸ª, å«åšk1
a1.sinks    = k1
# Agent a1 çš„ channelæœ‰ä¸€ä¸ª, å«åšc1
a1.channels = c1

# é…ç½® Agent a1çš„source r1çš„å±æ€§
a1.sources.r1.type = avro
a1.sources.r1.bind = 0.0.0.0
# ç›‘å¬çš„ç«¯å£
a1.sources.r1.port = 4545

# é…ç½® Agent a1çš„sink k1çš„å±æ€§
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = subowen
a1.sinks.k1.kafka.bootstrap.servers = bigdata:9092
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1
a1.sinks.k1.kafka.producer.compression.type = snappy

# é…ç½® Agent a1 çš„ channel c1 çš„å±æ€§, channelæ˜¯ç”¨æ¥ç¼“å†²Eventæ•°æ®çš„
# channel çš„ç±»å‹æ˜¯å†…å­˜ channel, é¡¾åæ€ä¹‰è¿™ä¸ª channel æ˜¯ä½¿ç”¨å†…å­˜æ¥ç¼“å†²æ•°æ®
a1.channels.c1.type = memory
# å†…å­˜channelçš„å®¹é‡å¤§å°æ˜¯1000, æ³¨æ„è¿™ä¸ªå®¹é‡ä¸æ˜¯è¶Šå¤§è¶Šå¥½, é…ç½®è¶Šå¤§, ä¸€æ—¦Flume æŒ‚æ‰ä¸¢å¤±çš„ event ä¹Ÿå°±è¶Šå¤š
a1.channels.c1.capacity = 1000
# source å’Œ sink ä»å†…å­˜ channel æ¯æ¬¡äº‹åŠ¡ä¼ è¾“çš„eventæ•°é‡
a1.channels.c1.transactionCapacity = 100

# æŠŠsourceå’Œsinkç»‘å®šåˆ°channelä¸Š
# ä¸source r1ç»‘å®šçš„channelæœ‰ä¸€ä¸ª, å«åšc1
a1.sources.r1.channels = c1
# ä¸sink k1ç»‘å®šçš„channelæœ‰ä¸€ä¸ª, å«åšc1
a1.sinks.k1.channel = c1
```
3. å¯åŠ¨ `webserver01` å’Œ `webserver02` çš„ Flumeï¼Œè¿™é‡Œä¸ºäº†ç®€åŒ–æ“ä½œï¼Œåªå¯åŠ¨ä¸€å°æœºå™¨ä¸Šçš„ Flume
```bash
[subowen@webserver01 config]$ ./start-flume.sh taildir-avro-stream.conf a1
```
4. å¯åŠ¨ `bigdata` ä¸Šçš„ Flume
```bash
[subowen@bigdata config]$ ./start-flume.sh avro-kafka-stream.conf a1
```
5. åœ¨ `bigdata` ä¸Šå¯åŠ¨ **`kafka-console-consumer`**ï¼Œä½¿ç”¨æ¶ˆè´¹è€…Shellè¿›è¡Œæµ‹è¯•æ¶ˆè´¹ï¼ŒåæœŸä¼šæ›´æ¢ä¸º ***`SparkStreaming`*** è¿›è¡Œæ¶ˆè´¹ã€‚
```bash
[subowen@bigdata ~]$ kafka-console-consumer.sh --topic subowen --bootstrap-server bigdata:9092
```
6. åœ¨ Windows ä¸‹è¿è¡Œå®¢æˆ·ç«¯ï¼Œæ¨¡æ‹Ÿæ•°æ®çš„äº§ç”Ÿï¼Œæ‰§è¡Œ`ScenicAPP`ä»£ç ï¼Œæ¨¡æ‹Ÿæ•°æ®çš„ç”Ÿæˆè¿‡ç¨‹ã€‚

```java
package com.example.x.data;

import com.example.x.data.producer.DataProducer;

public class ScenicApp {
    public static final String url = "http://192.168.26.111:9527/logweb/upload";
    public static void main(String[] args) throws Exception{
        DataProducer.producer(url);
    }
}
```

### ğŸ“ˆ 4. æ•°æ®å®æ—¶åˆ†æ

**Spark 3.0.1** å®˜æ–¹æ–‡æ¡£å…¥å£ï¼š[Overview - Spark 3.0.1 Documentation (apache.org)](https://spark.apache.org/docs/3.0.1/)

#### 4.1 `SparkStreaming` æ¦‚è¿°

`SparkStreaming` å®˜æ–¹æ–‡æ¡£å…¥å£ï¼š[Spark Streaming - Spark 3.0.1 Documentation (apache.org)](https://spark.apache.org/docs/3.0.1/streaming-programming-guide.html)

`SparkStreaming` ç±»ä¼¼äºä¹‹å‰å­¦ä¹ çš„ `SparkRDD` å’Œ`SparkSQL`ï¼Œæ˜¯ **Spark API** çš„æ ¸å¿ƒæ‰©å±•ï¼Œæ”¯æŒå®æ—¶æ•°æ®æµçš„å¯æ‰©å±•ã€é«˜ååé‡å’Œå®¹é”™ã€‚æ•°æ®å¯ä»¥ä»`Kafka`ã€`Flume`ã€`Kinesis`æˆ–`TCP Socket` ç­‰è®¸å¤šæ¥æºä¸­è¯»å–ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨å¤æ‚çš„ç®—æ³•è¿›è¡Œå¤„ç†ï¼Œè¿™äº›ç®—æ³•ç”¨é«˜çº§å‡½æ•°ï¼ˆå¦‚ `map`ã€`reduce`ã€`join` å’Œ `window`ï¼‰è¡¨ç¤ºã€‚æœ€åï¼Œå¤„ç†è¿‡çš„æ•°æ®å¯ä»¥ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿã€æ•°æ®åº“å’Œå®æ—¶ä»ªè¡¨æ¿ã€‚äº‹å®ä¸Šï¼Œæ‚¨å¯ä»¥åœ¨æ•°æ®æµä¸Šåº”ç”¨ **Spark** çš„æœºå™¨å­¦ä¹ å’Œå›¾å½¢å¤„ç†ç®—æ³•ã€‚

<img src="https://img-blog.csdnimg.cn/direct/92d09a65f9c04aa09233f322ec8fd1b9.jpeg#pic_center" alt="SparkStream" style="zoom:67%;" />

`SparkStreaming` æ˜¯ `Spark` ä¸­ç”¨äºå¤„ç†å®æ—¶æ•°æ®çš„ä¸€ä¸ªæ¨¡å—ã€‚

åœ¨å†…éƒ¨ï¼Œä»–çš„å·¥ä½œæµç¨‹æ˜¯ï¼š`SparkStreaming` æ¥æ”¶å®æ—¶è¾“å…¥çš„æ•°æ®æµï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œåˆ†æ‰¹ï¼ˆå¾®æ‰¹ï¼‰å¤„ç†ï¼Œç”± **Spark** å¼•æ“è¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆæœ€ç»ˆçš„æ‰¹é‡ç»“æœæµã€‚

<img src="https://img-blog.csdnimg.cn/direct/cf15f807ece04263a9a25833de244ee7.jpeg#pic_center" alt="SparkStreamFlow" style="zoom: 80%;" />

`SparkStreaming` æ˜¯å¾®æ‰¹å¤„ç†ï¼ˆæ‰¹æ¬¡ç‰¹åˆ«å°ï¼Œè¶³ä»¥å®ç°å®æ—¶å¤„ç†ï¼‰ï¼Œä¸æ˜¯çœŸæ­£çš„æµå¤„ç†ã€‚

è¿™é‡Œä¹Ÿå°±å¯ä»¥æ›´æ˜¾è‘—çš„å¾—åˆ°ç¦»çº¿æ‰¹æ•°æ®å’Œå®æ—¶æ•°æ®ä¹‹é—´çš„åŒºåˆ«ï¼š

- **ç¦»çº¿æ‰¹æ•°æ®ï¼š**boundâ€”â€”æœ‰ç•Œçš„æ•°æ®

- **å®æ—¶æ•°æ®ï¼š**unboundâ€”â€”æ— ç•Œçš„æ•°æ®

#### 4.2 å¼€å‘ç¬¬ä¸€ä¸ª`SparkStreming`æ¡ˆä¾‹

ç±»æ¯” `SparkRDD` å¼€å‘çš„æµç¨‹ï¼Œæˆ‘ä»¬ç»™å‡º`SparkStreaming` å¼€å‘çš„æµç¨‹ã€‚

`SparkRDD` ç¼–ç¨‹æ¨¡å‹ï¼š

1. åˆ›å»º `SparkContext`
2. è¯»å–æ•°æ®æº
3. å¤„ç†æ•°æ®
4. è¾“å‡ºç»“æœ
5. å…³é—­ `SparkContext`

`SparkStreaming` ç¼–ç¨‹æ¨¡å‹ï¼š

1. åˆ›å»º `StreamingContext`
2. è¯»å–æ•°æ®
3. å¤„ç†æ•°æ®
4. è¾“å‡ºç»“æœ
5. å¯åŠ¨ç¨‹åºï¼ˆé˜»å¡ï¼‰
6. ç­‰å¾…ç¨‹åºå…³é—­

ç”±æ­¤ï¼Œæˆ‘ä»¬å¼€å‘æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ª `SparkStreaming` å®ä¾‹ã€‚

1. åœ¨ IDEA ä¸­æ·»åŠ  Maven ä¾èµ–

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-streaming_2.12</artifactId>
        <version>3.0.1</version>
    </dependency>
</dependencies>
```

2. åˆ›å»º `TcpStreamingAPP`ï¼Œç¼–å†™ä»£ç 

```scala
package com.example.x.sparkstream

import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}

object TcpStreamingAPP {
    def main(args: Array[String]): Unit = {
        // ç¬¬ä¸€æ­¥: åˆ›å»º StreamingContext
        val conf = new SparkConf().setMaster("local[2]").setAppName("TcpStreaming")
        val streamingContext = new StreamingContext(conf, Seconds(1))

        // ç¬¬äºŒæ­¥: è¯»å–æ•°æ®: æ¥æ”¶ hostname:port å‘é€çš„æ•°æ®
        // SparkCore        SparkContext        RDD(å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†)â€”â€”ä¸å¯å˜çš„ã€å¯åˆ†åŒºçš„ã€å¯å¹¶è¡Œè®¡ç®—
        // SparkSQL         SparkSession        DataSet/DataFrame
        // SparkStreaming   StreamingContext    DStream
        val hostname = "bigdata"
        val port     = 9999
        var tcpDStream: ReceiverInputDStream[String] = streamingContext.socketTextStream(hostname, port)

        // ç¬¬ä¸‰æ­¥: å¤„ç†æ•°æ®
        val wcDStream: DStream[(String, Int)] = tcpDStream.flatMap(_.split(","))
          .map((_, 1))
          .reduceByKey(_ + _)   // é»˜è®¤è®¡ç®—çš„æ˜¯å½“å‰æ‰¹æ¬¡çš„æ•°æ®, ä¸ä¼šä¸ä¹‹å‰çš„æ‰¹æ¬¡è¿›è¡Œç´¯åŠ æ“ä½œ

        // ç¬¬å››æ­¥: ç´¯åŠ ç»“æœ
        wcDStream.print()

        // ç¬¬äº”æ­¥: å¯åŠ¨ç¨‹åº
        streamingContext.start()

        // ç¬¬å…­æ­¥: ç­‰å¾…ç¨‹åºå…³é—­
        streamingContext.awaitTermination()
    }
}
```

3. åœ¨ `bigdata` ä¸­å®‰è£…å’Œå¯åŠ¨ `netcat` ç”Ÿæˆæ•°æ®

```bash
[subowen@bigdata ~]$ sudo yum -y install nc
[subowen@bigdata ~]$ nc â€“lk 9999
```

4. å¯åŠ¨ç¨‹åºè¿›è¡Œæµ‹è¯•ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼šç¨‹åºåªè®¡ç®—å½“å‰æ—¶é—´ç‚¹å‘é€è¿‡æ¥çš„æ•°æ®ï¼ˆæ— çŠ¶æ€ï¼‰ã€‚

#### 4.3 `SparkStreaming` æ ¸å¿ƒæ¦‚å¿µ

| æ¦‚å¿µ                            | è§£é‡Š                                                         |
| ------------------------------- | ------------------------------------------------------------ |
| `StreamingContext`              | `SparkStreaming`åŠŸèƒ½çš„ä¸»è¦å…¥å£ç‚¹ï¼Œå®ƒæä¾›äº†ç”¨äºä»å„ç§è¾“å…¥æºåˆ›å»º `DStream` çš„æ–¹æ³•ã€‚<br>åˆ›å»ºå’Œè½¬æ¢`DStream`åï¼Œå¯ä»¥åˆ†åˆ«ä½¿ç”¨ `start()` å’Œ `stop()` å¯åŠ¨å’Œåœæ­¢æµè®¡ç®—ã€‚<br>`awaitTermination()` ç­‰å¾…æ‰§è¡Œåœæ­¢ï¼Œå…è®¸å½“å‰çº¿ç¨‹é€šè¿‡`stop()` æ‰‹åŠ¨åœæ­¢æˆ–é€šè¿‡ä¸€ä¸ªå¼‚å¸¸ç­‰å¾…`StreamingContext`çš„ç»ˆæ­¢ã€‚ |
| `DStream`                       | `DStream` æ˜¯ `SparkStreaming` æä¾›çš„åŸºæœ¬æŠ½è±¡ï¼Œå®ƒè¡¨ç¤ºä¸€ä¸ªè¿ç»­çš„æ•°æ®æµï¼Œè¦ä¹ˆæ˜¯ä»`Source`æ¥æ”¶çš„è¾“å…¥æ•°æ®æµï¼Œè¦ä¹ˆæ˜¯é€šè¿‡è½¬æ¢è¾“å…¥æµç”Ÿæˆçš„å¤„ç†æ•°æ®æµã€‚åœ¨å†…éƒ¨ï¼Œ`DStream`ç”±ä¸€ç³»åˆ—è¿ç»­çš„`RDD`è¡¨ç¤ºï¼Œ`DStream`ä¸­çš„æ¯ä¸ª`RDD`éƒ½åŒ…å«ä¸€å®šæ—¶é—´é—´éš”çš„æ•°æ®ã€‚<br><img src="https://img-blog.csdnimg.cn/direct/0e07b9cf6c61446290e8e6cae12b3316.jpeg#pic_center" style="zoom:50%;" /><br>åœ¨`DStream`ä¸Šåº”ç”¨çš„ä»»ä½•æ“ä½œéƒ½è½¬æ¢ä¸ºåœ¨åº•å±‚ `RDD`ä¸Šçš„æ“ä½œã€‚<br><img src="https://img-blog.csdnimg.cn/direct/45176000102c4e97bcfce70bf868d4f1.jpeg#pic_center" style="zoom:50%;" /><br>`DStream`ä¸­æ˜¯ç”±æ¯ä¸ªæ‰¹æ¬¡ç”Ÿæˆçš„`RDD`ç»„æˆçš„ã€‚ |
| `Input DStreams` å’Œ` Receivers` | `Input DStreams`æ˜¯è¡¨ç¤ºä»æ•°æ®æºæ¥æ”¶çš„è¾“å…¥æ•°æ®æµçš„`DStreams`ã€‚æ¯ä¸ª`Input DStream`ï¼ˆæ–‡ä»¶æµé™¤å¤–ï¼‰éƒ½ä¸ä¸€ä¸ª`Receiver`å¯¹è±¡ç›¸å…³è”ï¼Œæ¥æ”¶æ¥è‡ªæºçš„æ•°æ®å¹¶å°†å…¶å­˜å‚¨åœ¨`Spark`çš„å†…å­˜ä¸­è¿›è¡Œå¤„ç†ã€‚ |

#### 4.4 æ•°æ®æº

æ•°æ®æºå…¶å®å°±æ˜¯ä»å“ªé‡Œè¯»å–æ•°æ®ï¼ŒåŒºåˆ«å°±åœ¨äºä¸€äº›è¯»å–çš„å†™æ³•æœ‰ä¸åŒã€‚

##### 4.4.1 åŸºæœ¬æ•°æ®æº

- `Socket`å¥—æ¥å­—æ•°æ®æºï¼š`socketTextStream()`ã€‚

- `File System`æ–‡ä»¶ç³»ç»Ÿæ•°æ®æºï¼š`textFileStream()`ã€‚

##### 4.4.2  é«˜çº§æ•°æ®æº

`Spark Kafka`å®˜æ–¹æ–‡æ¡£ï¼š[Spark Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher) - Spark 3.0.1 Documentation (apache.org)](https://spark.apache.org/docs/3.0.1/streaming-kafka-0-10-integration.html)

- é«˜çº§æ•°æ®æºï¼šå¦‚ `kafka` ã€`Kinesis`ç­‰ã€‚
- ç”¨ä»£ç å¯¹æ¥ `Kafka`

```scala
package com.example.x.sparkstream

import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.codehaus.jackson.map.deser.std.StringDeserializer


object KafkaStreamApp {
    def main(args: Array[String]): Unit = {
        // ç¬¬ä¸€æ­¥
        // Spark ä¸­é‡åˆ°çš„ä¸€åˆ‡åºåˆ—åŒ–é—®é¢˜éƒ½éœ€è¦ KryoSerializer
        val conf: SparkConf = new SparkConf().setMaster("Local[2]").setAppName("KafkaStreaming").set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        val streamingContext = new StreamingContext(conf, Seconds(5))

        val kafkaParams = Map[String, Object] (
            ""->"",
            "bootstrap.servers"     -> "bigdata:9092",
            "key.deserializer"      -> classOf[StringDeserializer],
            "value.deserializer"    -> classOf[StringDeserializer],
            "group.id"              -> "subowen",
            "auto.offset,reset"     -> "latest",
            "enable.auto.commit"    -> (false: java.lang.Boolean)       // è‡ªåŠ¨æäº¤ offset(è‡ªåŠ¨è®°å½•è¯»åˆ°topicä¸­çš„å“ªä¸ªéƒ¨ä½)
        )
        val topics = "subowen"
        val kafkaStream = KafkaUtils.createDirectStream[String, String](
            streamingContext,
            PreferConsistent,
            Subscribe[String, String](topics, kafkaParams)	// æŒ‡å®šè®¢é˜…çš„ Topic
        )
        
        kafkaStream.flatMap(x=>x.value().split(",")).map((_, 1)).reduceByKey(_ + _).print()

        // å¯åŠ¨ StreamingContext
        streamingContext.start()

        // é˜»å¡ StreamingContext
        streamingContext.awaitTermination()
    }
}
```

##### 4.4.3 ç»´æŠ¤ Kafka Offset

åœ¨å‰é¢çš„å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å·²ç»äº†è§£äº†ä¸¤ç§æ¶ˆè´¹è€…çš„æ¶ˆè´¹ç­–ç•¥ï¼Œåˆ†åˆ«æ˜¯ `Lastest` å’Œ `Earliest`ï¼Œè¿™ä¸¤ç§æ¶ˆè´¹ç­–ç•¥éƒ½æ˜¯å­˜åœ¨é—®é¢˜çš„ã€‚

- `Lastest` æ¶ˆè´¹ç­–ç•¥ï¼šå½“æ¶ˆè´¹è€…å¯åŠ¨ä¹‹åï¼Œä»å¯åŠ¨åäº§ç”Ÿçš„ç¬¬ä¸€æ¡æ•°æ®å¼€å§‹æ¶ˆè´¹
  - æ¶ˆè´¹è€…ç¬¬ä¸€æ¬¡å¯åŠ¨ä¹‹å‰ï¼Œ`topic`ä¸­å·²ç»å­˜åœ¨çš„æ•°æ®æ˜¯ä¸ä¼šè¢«æ¶ˆè´¹ã€‚
  - æ¶ˆè´¹è€…å®•æœºçš„æ—¶é—´æ®µå†…ï¼Œ`topic`ä¸­äº§ç”Ÿçš„æ•°æ®ä¸ä¼šè¢«æ¶ˆè´¹ã€‚

- `Earliest` æ¶ˆè´¹ç­–ç•¥ï¼š



å¦‚ä½•æ‰‹åŠ¨ç»´æŠ¤**Kafka Offset**ï¼Ÿ

æˆ‘ä»¬å¯ä»¥å°† **Offset** æŒä¹…åŒ–åˆ°ä¸€ä¸ªæ•°æ®åº“ä¸­ï¼Œå¦‚`MySQL`ã€`HDFS`ã€`ZooKeeper` ä¸­ã€‚ä¸‹é¢å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ `MySQL` æ¥ç»´æŠ¤ `Kafka Offset`ã€‚

1. è®¾è®¡è¡¨ç»“æ„

|   å­—æ®µå    |   çº¦æŸ   |    ç±»å‹     |         å¤‡æ³¨         |
| :---------: | :------: | :---------: | :------------------: |
|   k_topic   | è”åˆä¸»é”® | VARCHAR(50) | å¯ä»¥ç›´æ¥ä»ä»£ç ä¸­è·å¾— |
|  k_groupid  | è”åˆä¸»é”® | VARCHAR(50) | å¯ä»¥ç›´æ¥ä»ä»£ç ä¸­è·å¾— |
| k_partition | è”åˆä¸»é”® |     INT     |          -           |
|  k_offset   |    -     |   BIGINT    |          -           |

2. å®ç°è¡¨ç»“æ„

```sql
CREATE TABLE IF NOT EXISTS t_offset (
    k_topic       VARCHAR(50)   NOT NULL,           -- è®¾ç½® Topic
    k_groupid     VARCHAR (50)  NOT NULL,           -- è®¾ç½® groupid
    k_partition   INT           NOT NULL,           -- è®¾ç½® Partition
    k_offset      BIGINT        NOT NULL,           -- è®¾ç½® offset
    PRIMARY KEY(k_topic, k_groupid, k_partition)    -- è®¾ç½®è”åˆä¸»é”®
);
```

3. å¼€å‘å·¥å…·ç±»


```scala
package com.example.x.utils

import java.sql.{Connection, PreparedStatement, ResultSet}

import org.apache.kafka.common.TopicPartition
import org.apache.spark.streaming.kafka010.OffsetRange

import scala.collection.mutable.ListBuffer

object KafkaOffsetManagerUtils {
    // ä¿å­˜ Offset
    def saveOffset(offsetRanges: Array[OffsetRange], groupId: String) = {
        val connection: Connection = ConnectionUtils.getConnection()
        val sql =
            """
              |INSERT INTO t_offset(k_topic, k_groupid, k_partition, k_offset) VALUES(?, ?, ?, ?)
              |ON DUPLICATE KEY UPDATE k_offset=?
              |""".stripMargin
        val pst: PreparedStatement = connection.prepareStatement(sql)
        offsetRanges.map(offsetRanges => {
            val topic       = offsetRanges.topic
            val partition   = offsetRanges.partition
            val offset      = offsetRanges.untilOffset
            pst.setString(1, topic)
            pst.setString(2, groupId)
            pst.setInt(3, partition)
            pst.setLong(4, offset)
            pst.setLong(5, offset)
            pst.execute()
        })
        ConnectionUtils.closeConnection(connection)
    }

    // è¯» Offset
    def readOffset(topic: String, groupId: String): Map[TopicPartition, Long] = {
        val connection: Connection = ConnectionUtils.getConnection()
        val sql = "SELECT k_topic, k_groupid, k_partition, k_offset FROM t_offset WHERE k_topic=? AND k_groupid=?"
        val pst: PreparedStatement = connection.prepareStatement(sql)
        pst.setString(1, topic)
        pst.setString(2, groupId)
        val resultSet: ResultSet = pst.executeQuery()
        val list = new ListBuffer[(TopicPartition, Long)]
        while(resultSet.next()) {
            val partition = resultSet.getInt("k_partition")
            val topicPartition = new TopicPartition(topic, partition)
            val offset = resultSet.getLong("k_offset")
            list.append((topicPartition, offset))
        }
        list.toMap
    }
}
```

4. ä»£ç æ¡ˆä¾‹

```scala
package com.example.x.quickstart

import java.sql.{Connection, PreparedStatement}

import com.example.x.utils.{ConnectionUtils, KafkaOffsetManagerUtils}
import org.apache.kafka.common.TopicPartition
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.{SparkConf, TaskContext}
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.streaming.kafka010.{HasOffsetRanges, KafkaUtils, OffsetRange}
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.{Seconds, StreamingContext}

object KafkaOffset {
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount").set("spark.serializer", "org.apache.spark.serializer.KryoSerializer").set("spark.port.maxRetries", "100")
        val streamingContext = new StreamingContext(conf, Seconds(5))

        val topics = Array("KafkaOffset")
        val groupId = "szy"

        val kafkaParams = Map[String, Object] (
            ""->"",
            "bootstrap.servers"     -> "bigdata:9092",
            "key.deserializer"      -> classOf[StringDeserializer],
            "value.deserializer"    -> classOf[StringDeserializer],
            "group.id"              -> groupId,
            "auto.offset,reset"     -> "earliest",
            "enable.auto.commit"    -> (false: java.lang.Boolean)       // è‡ªåŠ¨æäº¤ offset(è‡ªåŠ¨è®°å½•è¯»åˆ°topicä¸­çš„å“ªä¸ªéƒ¨ä½)
        )


        // å…ˆè¯»å–æ•°æ®åº“çš„ Offset, è®¾ç½®åˆ° Subscribe
        val offset: Map[TopicPartition, Long] = KafkaOffsetManagerUtils.readOffset(topics(0), groupId)
        println(">>> [LOGS] The Offset Now Reading: " + offset)

        val kafkaStream = KafkaUtils.createDirectStream[String, String](
            streamingContext,
            PreferConsistent,
            Subscribe[String, String](topics, kafkaParams, offset)  // æŒ‡å®šè®¢é˜…çš„ Topic
        )

        // æµ‹è¯•æ•°æ®:
        // a,a,a,a,a
        // b,b,b,c
        // c,c,d
        kafkaStream.foreachRDD { rdd =>
            val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
            rdd.foreachPartition { iter =>
                val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)
                /**
                 * @param: o.topic:        ä» topics å¯ä»¥è·å–åˆ°
                 * @param: o.partition:    åˆ†åŒº
                 * @param: o.fromOffset:   æ­£åœ¨æ‰§è¡Œçš„ Offset
                 * @param: o.untilOffset:  å³å°†æ‰§è¡Œçš„ Offset
                 */
                println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
                KafkaOffsetManagerUtils.saveOffset(offsetRanges, groupId)
            }
        }

        kafkaStream.print()

        streamingContext.start()
        streamingContext.awaitTermination()
    }
}
```

5. æµ‹è¯•

#### 4.5 è½¬æ¢æ“ä½œ

#### 4.6 è¾“å‡ºæ“ä½œ

å°†æ•°æ®è¾“å‡ºåˆ°æŒ‡å®šçš„æ•°æ®ä»“åº“ä¸­ï¼Œå¦‚`MySQL`ã€`Redis`ã€`HBase`ç­‰ã€‚ä¸‹é¢é€šè¿‡ä¸€ä¸ªç®€å•çš„ `WordCount` æ¡ˆä¾‹å¿«é€Ÿäº†è§£å¦‚ä½•å°†æ•°æ®å†™å…¥ `MySQL`ã€‚æœ¬æ¡ˆä¾‹å€ŸåŠ©

- åˆ›å»ºæ•°æ®åº“å’Œè¡¨

```sql
CREATE DATABASE travel CHARSET utf8;
CREATE TABLE wc(
    id 		BIGINT PRIMARY KEY AUTO_INCREMENT,
    word 	VARCHAR(50),
    count 	BIGINT
 );
```

- å­˜æ”¾ `Druid` çš„é…ç½®æ–‡ä»¶

```properties
url             =   jdbc:mysql://bigdata:3306/travel?useSSL=false&characterEncoding=UTF-8
username        =   root
password        =   root
driverClassName =   com.mysql.jdbc.Driver
initialSize     =   5
maxActive       =   20
minIdle         =   1
maxWait         =   60000
validationQuery =   SELECT 1
testOnBorrow    =   true
testWhileIdle   =   true
```

- ç¼–å†™æ•°æ®åº“è¿æ¥å·¥å…·ç±»`ConnectionUtils.scala`

```scala
package com.example.x.utils

import java.io.InputStream
import java.sql.Connection
import java.util.Properties

import com.alibaba.druid.pool.DruidDataSourceFactory
import javax.sql.DataSource

object ConnectionUtils {
    // 1. åˆ›å»º Druid çš„ DataSource å¯¹è±¡
    val dataSource: DataSource = {
        val properties = new Properties()
        val inputStream: InputStream = getClass.getClassLoader.getResourceAsStream("druid.properties")
        properties.load(inputStream)
        println(properties)
        DruidDataSourceFactory.createDataSource(properties)
    }

    // 2. åˆ›å»ºè·å–è¿æ¥çš„æ–¹æ³•, å‘å¤–æä¾›æ•°æ®åº“è¿æ¥
    def getConnection(): Connection = {
        dataSource.getConnection()
    }

    // 3. åˆ›å»ºè¿æ¥å›æ”¶æ–¹æ³•
    def closeConnection(connection: Connection): Unit = {
        if(null != connection) {
            connection.close()
        }
    }

    def main(args: Array[String]): Unit = {
        println(dataSource)
    }
}
```

- ç¼–å†™ `WordCount` ä»£ç 

```scala
package com.example.x.quickstart

import java.sql.{Connection, PreparedStatement}

import com.example.x.utils.ConnectionUtils
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent

object WordCount {
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("WordCount").set("spark.serializer", "org.apache.spark.serializer.KryoSerializer").set("spark.port.maxRetries", "100")
        val streamingContext = new StreamingContext(conf, Seconds(5))

        val kafkaParams = Map[String, Object] (
            ""->"",
            "bootstrap.servers"     -> "bigdata:9092",
            "key.deserializer"      -> classOf[StringDeserializer],
            "value.deserializer"    -> classOf[StringDeserializer],
            "group.id"              -> "WordCount",
            "auto.offset,reset"     -> "latest",
            "enable.auto.commit"    -> (false: java.lang.Boolean)       // è‡ªåŠ¨æäº¤ offset(è‡ªåŠ¨è®°å½•è¯»åˆ°topicä¸­çš„å“ªä¸ªéƒ¨ä½)
        )

        val topics = Array("WordCount")

        val kafkaStream = KafkaUtils.createDirectStream[String, String](
            streamingContext,
            PreferConsistent,
            Subscribe[String, String](topics, kafkaParams)
        )

        val wcDStream: DStream[(String, Int)] = kafkaStream.flatMap(x=>x.value().split(",")).map((_, 1)).reduceByKey(_ + _)

        // æµ‹è¯•æ•°æ®ï¼š
        // a,b,a,c,a,d,c,c,a,m,e,f
        // (a, 4)
        // (b, 1)
        // (c, 3)
        // (d, 1)
        // (e, 1)
        // (m, 1)
        // (f, 1)
        wcDStream.foreachRDD(rdd =>
            rdd.foreachPartition { partitionOfRecords =>
                val connection: Connection = ConnectionUtils.getConnection()
                val sql = "INSERT INTO wordcount(word, count) VALUES (?, ?)"    // åœ¨æ‰§è¡Œè¿™ä¸ªSQLè¯­å¥ä¹‹å‰éœ€è¦åˆ›å»º `wordcount` è¡¨
                val pst: PreparedStatement = connection.prepareStatement(sql)
                partitionOfRecords.foreach(record => {
                    pst.setString(1, record._1)
                    pst.setLong(2, record._2)
                    pst.execute()
                })
                if(pst != null) {
                    pst.close()
                }
                ConnectionUtils.closeConnection(connection)
            }
        )

        streamingContext.start()
        streamingContext.awaitTermination()
    }
}
```

#### 4.7 ä»»åŠ¡å®æ–½

1. å¯åŠ¨ `WebServer01` çš„ `Flume`

```bash
[subowen@webserver01 config]$ ./start-flume.sh taildir-avro-stream.conf a1
```

2. å¯åŠ¨ `Bigdata` çš„ `Flume`

```bash
[subowen@bigdata config]$ ./start-flume.sh taildir-avro-stream.conf a1
```

3. å¯åŠ¨ `WebServer01` çš„ç”Ÿäº§è€…`Producer`ï¼Œå³`logweb-1.0.jar`

```bash
[subowen@webserver01 serverJar]$ ll
æ€»ç”¨é‡ 17608
drwxrwxr-x. 3 subowen subowen       18 6æœˆ  11 12:10 logweb
-rw-rw-r--. 1 subowen subowen 18014926 6æœˆ  11 12:08 logweb-1.0.jar
-rw-------. 1 subowen subowen     5531 6æœˆ  12 13:41 nohup.out
-rwxrw-r--. 1 subowen subowen       33 6æœˆ  11 12:09 start-logweb.sh
[subowen@webserver01 serverJar]$ cat ./start-logweb.sh
nohup java -jar logweb-1.0.jar &
[subowen@webserver01 serverJar]$ ./start-logweb.sh
```

4. ç¼–å†™æ¶ˆè´¹è€…ç¨‹åºï¼Œè¿›è¡Œå®æ—¶æ•°æ®å¤„ç†ã€‚

```scala
package com.example.x.sparkstream

import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent

object RealtimeScenicPA {
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("RealtimeScenic").set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        val streamingContext = new StreamingContext(conf, Seconds(5))

        val kafkaParams = Map[String, Object] (
            ""->"",
            "bootstrap.servers"     -> "bigdata:9092",
            "key.deserializer"      -> classOf[StringDeserializer],
            "value.deserializer"    -> classOf[StringDeserializer],
            "group.id"              -> "subowen",
            "auto.offset,reset"     -> "latest",
            "enable.auto.commit"    -> (false: java.lang.Boolean)       // è‡ªåŠ¨æäº¤ offset(è‡ªåŠ¨è®°å½•è¯»åˆ°topicä¸­çš„å“ªä¸ªéƒ¨ä½)
        )

        val topics = Array("subowen")

        val kafkaStream = KafkaUtils.createDirectStream[String, String](
            streamingContext,
            PreferConsistent,
            Subscribe[String, String](topics, kafkaParams)
        )

        // 1. ç»Ÿè®¡ç›¸åŒç»çº¬åº¦å’Œæ—¶é—´çš„äººæµé‡æ•°æ®
        val peopleCountPerLocationAndTime: DStream[(String, Int)] = kafkaStream.map(_.value)
          .map(line => {
              val splitData = line.split(",")
              (s"${splitData(0)},${splitData(1)},${splitData(2)},${splitData(3)}", 1)
          })
          .reduceByKey(_ + _)

        // 2. åŸºäºçŠ¶æ€æ“ä½œç»Ÿè®¡æ¯ä¸ªæ™¯ç‚¹æ¯åˆ†é’Ÿçš„äººæµé‡
        val peopleCountPerMinutePerLocation = peopleCountPerLocationAndTime.map(line => {
            val keySplit = line._1.split(",")
            (s"${keySplit(2)},${keySplit(3).substring(0, 12)}", line._2)
        }).reduceByKey(_ + _)

        // è¾“å‡ºç»“æœ
        peopleCountPerLocationAndTime.print()
        peopleCountPerMinutePerLocation.print()

        // å¯åŠ¨ StreamingContext
        streamingContext.start()

        // é˜»å¡ StreamingContext
        streamingContext.awaitTermination()
    }
}
```

5. åœ¨ Windows å¹³å°è¿è¡Œ**ç”Ÿäº§è€…ç¨‹åº**`ScenicAPP`å’Œ**æ¶ˆè´¹è€…ç¨‹åº**`RealtimeScenicPA`ã€‚ç»“æœå¦‚ä¸‹ï¼š

<img src="https://img-blog.csdnimg.cn/direct/92f5069cf4fd4e859106d0c07633b718.jpeg#pic_center" style="zoom:50%;" />

<img src="https://img-blog.csdnimg.cn/direct/1f33579ba7e54ee185805243ab008ead.jpeg#pic_center" style="zoom:50%;" />

6. ç»“åˆ `MySQL` è¿›è¡Œæ•°æ®æŒä¹…åŒ–ï¼Œé¦–å…ˆå»ºç«‹å¥½æ•°æ®åº“å’Œæ•°æ®è¡¨ã€‚

```sql
-- SQL å»ºè¡¨è¯­å¥å¦‚ä¸‹
CREATE DATABASE IF NOT EXISTS travel CHARSET UTF8;

USE travel;

CREATE TABLE IF NOT EXISTS people_count_per_location_and_time (
    -- id           BIGINT PRIMARY KEY AUTO_INCREMENT,  -- id
    longitude       DOUBLE,                             -- ç»åº¦
    latitude        DOUBLE,                             -- çº¬åº¦
    scenic          VARCHAR(20),                        -- æ™¯ç‚¹
    sec_moment      VARCHAR(15),                        -- å…·ä½“æ—¶åˆ»
    sec_quantity    BIGINT,                             -- å…·ä½“æ•°é‡
    PRIMARY KEY(scenic, sec_moment)                     -- è®¾ç½®è”åˆä¸»é”®
);

CREATE TABLE IF NOT EXISTS people_count_per_minute_per_location (
    -- id           BIGINT PRIMARY KEY AUTO_INCREMENT,  -- id
    scenic          VARCHAR(20),                        -- æ™¯ç‚¹
    min_moment      VARCHAR(15),                        -- å…·ä½“æ—¶åˆ»
    min_quantity    BIGINT,                             -- å…·ä½“æ•°é‡
    PRIMARY KEY(scenic, min_moment)                     -- è®¾ç½®è”åˆä¸»é”®
);
```

7. é›†åˆä¹‹å‰ `WordCount` æ¡ˆä¾‹ä¸­ç¼–å†™ `ConnectionUtils.scala` ä»£ç ï¼Œå®Œæˆè¯¥é¡¹ç›®çš„æ•°æ®æŒä¹…åŒ–ä»»åŠ¡ã€‚ç¼–å†™`RealtimeScenicPA.scala`ä»£ç å¦‚ä¸‹ï¼š

```scala
package com.example.x.travel

import java.sql.{Connection, PreparedStatement}

import com.example.x.utils.ConnectionUtils
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.{Seconds, StreamingContext}

object RealtimeScenicPA {
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("RealtimeScenic").set("spark.serializer", "org.apache.spark.serializer.KryoSerializer").set("spark.port.maxRetries", "100")
        val streamingContext = new StreamingContext(conf, Seconds(5))
        streamingContext.checkpoint("ckpt")

        val kafkaParams = Map[String, Object] (
            ""->"",
            "bootstrap.servers"     -> "bigdata:9092",
            "key.deserializer"      -> classOf[StringDeserializer],
            "value.deserializer"    -> classOf[StringDeserializer],
            "group.id"              -> "subowen",
            "auto.offset,reset"     -> "latest",
            "enable.auto.commit"    -> (false: java.lang.Boolean)       // è‡ªåŠ¨æäº¤ offset(è‡ªåŠ¨è®°å½•è¯»åˆ°topicä¸­çš„å“ªä¸ªéƒ¨ä½)
        )

        val topics = Array("subowen")

        val kafkaStream = KafkaUtils.createDirectStream[String, String](
            streamingContext,
            PreferConsistent,
            Subscribe[String, String](topics, kafkaParams)
        )

        val updateFunc = (newValues: Seq[Int], state: Option[Int]) => {
            val currentCount = newValues.sum
            val previousCount = state.getOrElse(0)
            val newCount = currentCount + previousCount
            Some(newCount)
        }

        // 1. ç»Ÿè®¡ç›¸åŒç»çº¬åº¦å’Œæ—¶é—´çš„äººæµé‡æ•°æ®
        val peopleCountPerLocationAndTime: DStream[(String, Int)] = kafkaStream.map(_.value)
          .map(line => {
              val splitData = line.split(",")
              (s"${splitData(0)},${splitData(1)},${splitData(2)},${splitData(3)}", 1)
          })
          .updateStateByKey(updateFunc)

        // 2. åŸºäºçŠ¶æ€æ“ä½œç»Ÿè®¡æ¯ä¸ªæ™¯ç‚¹æ¯åˆ†é’Ÿçš„äººæµé‡
        val peopleCountPerMinutePerLocation = peopleCountPerLocationAndTime.map(line => {
            val keySplit = line._1.split(",")
            (s"${keySplit(2)},${keySplit(3).substring(0, 12)}", line._2)
        }).reduceByKey(_ + _)

        // è¾“å‡ºç»“æœ
        peopleCountPerLocationAndTime.foreachRDD(rdd =>
            rdd.foreachPartition { partitionOfRecords =>
                val connection: Connection = ConnectionUtils.getConnection()
                val sql =
                    """
                      |INSERT INTO people_count_per_location_and_time(longitude, latitude, scenic, sec_moment, sec_quantity)
                      |VALUES (?, ?, ?, ?, ?)
                      |ON DUPLICATE KEY
                      |UPDATE sec_quantity = ?""".stripMargin    // åœ¨æ‰§è¡Œè¿™ä¸ªSQLè¯­å¥ä¹‹å‰éœ€è¦åˆ›å»ºè¡¨
                val pst: PreparedStatement = connection.prepareStatement(sql)
                partitionOfRecords.foreach(record => {
                    val splitRecord = record._1.split(",")
                    // println(splitRecord(0) + " " + splitRecord(1) + " " + splitRecord(2) + " " + splitRecord(3))
                    pst.setDouble(1, splitRecord(0).toDouble)
                    pst.setDouble(2, splitRecord(1).toDouble)
                    pst.setString(3, splitRecord(2))
                    pst.setString(4, splitRecord(3))
                    pst.setInt(5, record._2)
                    pst.setInt(6, record._2)
                    pst.execute()
                })
                if(pst != null) {
                    pst.close()
                }
                ConnectionUtils.closeConnection(connection)
            }
        )

        peopleCountPerMinutePerLocation.foreachRDD(rdd =>
            rdd.foreachPartition { partitionOfRecords =>
                val connection: Connection = ConnectionUtils.getConnection()
                val sql =
                    """
                      |INSERT INTO people_count_per_minute_per_location(scenic, min_moment, min_quantity)
                      |VALUES (?, ?, ?)
                      |ON DUPLICATE KEY
                      |UPDATE min_quantity = ?""".stripMargin    // åœ¨æ‰§è¡Œè¿™ä¸ªSQLè¯­å¥ä¹‹å‰éœ€è¦åˆ›å»ºè¡¨
                val pst: PreparedStatement = connection.prepareStatement(sql)
                partitionOfRecords.foreach(record => {
                    val splitRecord = record._1.split(",")
                    // println(splitRecord(0) + " " + splitRecord(1) + " " + splitRecord(2) + " " + splitRecord(3))
                    pst.setString(1, splitRecord(0))
                    pst.setString(2, splitRecord(1))
                    pst.setInt(3, record._2)
                    pst.setInt(4, record._2)
                    pst.execute()
                })
                if(pst != null) {
                    pst.close()
                }
                ConnectionUtils.closeConnection(connection)
            }
        )

        // peopleCountPerLocationAndTime.print()
         peopleCountPerMinutePerLocation.print()

        // å¯åŠ¨ StreamingContext
        streamingContext.start()

        // é˜»å¡ StreamingContext
        streamingContext.awaitTermination()
    }
}
```

8. ç»“åˆ `MySQL` å¯¹ `t_offset` è¡¨è¿›è¡Œç»´æŠ¤ã€‚

```scala
package com.example.x.travel

import java.sql.{Connection, PreparedStatement}

import com.example.x.utils.{ConnectionUtils, KafkaOffsetManagerUtils}
import org.apache.kafka.common.TopicPartition
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.{SparkConf, TaskContext}
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.streaming.kafka010.{HasOffsetRanges, KafkaUtils, OffsetRange}
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.{Seconds, StreamingContext}

object RealtimeScenicPA {
    def main(args: Array[String]): Unit = {
        val conf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("RealtimeScenic").set("spark.serializer", "org.apache.spark.serializer.KryoSerializer").set("spark.port.maxRetries", "100")
        val streamingContext = new StreamingContext(conf, Seconds(5))
        streamingContext.checkpoint("ckpt")

        val topics  = Array("subowen")
        val groupId = "subowen"

        val kafkaParams = Map[String, Object] (
            ""->"",
            "bootstrap.servers"     -> "bigdata:9092",
            "key.deserializer"      -> classOf[StringDeserializer],
            "value.deserializer"    -> classOf[StringDeserializer],
            "group.id"              -> groupId,
            "auto.offset,reset"     -> "earliest",
            "enable.auto.commit"    -> (false: java.lang.Boolean)       // è‡ªåŠ¨æäº¤ offset(è‡ªåŠ¨è®°å½•è¯»åˆ°topicä¸­çš„å“ªä¸ªéƒ¨ä½)
        )

        // å…ˆè¯»å–æ•°æ®åº“çš„ Offset, è®¾ç½®åˆ° Subscribe
        val offset: Map[TopicPartition, Long] = KafkaOffsetManagerUtils.readOffset(topics(0), groupId)
        println(">>> [LOGS] The Offset Now Reading: " + offset)

        val kafkaStream = KafkaUtils.createDirectStream[String, String](
            streamingContext,
            PreferConsistent,
            Subscribe[String, String](topics, kafkaParams, offset)
        )

        val updateFunc = (newValues: Seq[Int], state: Option[Int]) => {
            val currentCount = newValues.sum
            val previousCount = state.getOrElse(0)
            val newCount = currentCount + previousCount
            Some(newCount)
        }

        // 1. ç»Ÿè®¡ç›¸åŒç»çº¬åº¦å’Œæ—¶é—´çš„äººæµé‡æ•°æ®
        val peopleCountPerLocationAndTime: DStream[(String, Int)] = kafkaStream.map(_.value)
          .map(line => {
              val splitData = line.split(",")
              (s"${splitData(0)},${splitData(1)},${splitData(2)},${splitData(3)}", 1)
          })
          .updateStateByKey(updateFunc)

        // 2. åŸºäºçŠ¶æ€æ“ä½œç»Ÿè®¡æ¯ä¸ªæ™¯ç‚¹æ¯åˆ†é’Ÿçš„äººæµé‡
        val peopleCountPerMinutePerLocation = peopleCountPerLocationAndTime.map(line => {
            val keySplit = line._1.split(",")
            (s"${keySplit(2)},${keySplit(3).substring(0, 12)}", line._2)
        }).reduceByKey(_ + _)

        // è¾“å‡ºç»“æœ
        peopleCountPerLocationAndTime.foreachRDD(rdd =>
            rdd.foreachPartition { partitionOfRecords =>
                val connection: Connection = ConnectionUtils.getConnection()
                val sql =
                    """
                      |INSERT INTO t_heat(longitude, latitude, scenic, sec_moment, sec_quantity)
                      |VALUES (?, ?, ?, ?, ?)
                      |ON DUPLICATE KEY
                      |UPDATE sec_quantity = ?""".stripMargin    // åœ¨æ‰§è¡Œè¿™ä¸ªSQLè¯­å¥ä¹‹å‰éœ€è¦åˆ›å»ºè¡¨
                val pst: PreparedStatement = connection.prepareStatement(sql)
                partitionOfRecords.foreach(record => {
                    val splitRecord = record._1.split(",")
                    // println(splitRecord(0) + " " + splitRecord(1) + " " + splitRecord(2) + " " + splitRecord(3))
                    pst.setDouble(1, splitRecord(0).toDouble)
                    pst.setDouble(2, splitRecord(1).toDouble)
                    pst.setString(3, splitRecord(2))
                    pst.setString(4, splitRecord(3))
                    pst.setInt(5, record._2)
                    pst.setInt(6, record._2)
                    pst.execute()
                })
                if(pst != null) {
                    pst.close()
                }
                ConnectionUtils.closeConnection(connection)
            }
        )

        peopleCountPerMinutePerLocation.foreachRDD(rdd =>
            rdd.foreachPartition { partitionOfRecords =>
                val connection: Connection = ConnectionUtils.getConnection()
                val sql =
                    """
                      |INSERT INTO t_scenic(scenic, min_moment, min_quantity)
                      |VALUES (?, ?, ?)
                      |ON DUPLICATE KEY
                      |UPDATE min_quantity = ?""".stripMargin    // åœ¨æ‰§è¡Œè¿™ä¸ªSQLè¯­å¥ä¹‹å‰éœ€è¦åˆ›å»ºè¡¨
                val pst: PreparedStatement = connection.prepareStatement(sql)
                partitionOfRecords.foreach(record => {
                    val splitRecord = record._1.split(",")
                    // println(splitRecord(0) + " " + splitRecord(1) + " " + splitRecord(2) + " " + splitRecord(3))
                    pst.setString(1, splitRecord(0))
                    pst.setString(2, splitRecord(1))
                    pst.setInt(3, record._2)
                    pst.setInt(4, record._2)
                    pst.execute()
                })
                if(pst != null) {
                    pst.close()
                }
                ConnectionUtils.closeConnection(connection)
            }
        )

        // ç»´æŠ¤ KafkaStream çš„ Offset è¡¨
        kafkaStream.foreachRDD { rdd =>
            val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
            rdd.foreachPartition { iter =>
                val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)
                /**
                 * @param: o.topic:        ä» topics å¯ä»¥è·å–åˆ°
                 * @param: o.partition:    åˆ†åŒº
                 * @param: o.fromOffset:   æ­£åœ¨æ‰§è¡Œçš„ Offset
                 * @param: o.untilOffset:  å³å°†æ‰§è¡Œçš„ Offset
                 */
                println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
                KafkaOffsetManagerUtils.saveOffset(offsetRanges, groupId)
            }
        }

        // peopleCountPerLocationAndTime.print()
        peopleCountPerMinutePerLocation.print()

        // å¯åŠ¨ StreamingContext
        streamingContext.start()

        // é˜»å¡ StreamingContext
        streamingContext.awaitTermination()
    }
}
```

### ğŸ¥ 5. æ•°æ®å®æ—¶ç›‘æ§ç³»ç»Ÿ

#### 5.1 è½¯ä»¶å¼€å‘ä½“ç³»æ¶æ„

##### 5.1.1 å¼€å‘æ¶æ„

| æ¶æ„                     | è¯´æ˜                                   |
| ------------------------ | -------------------------------------- |
| B/Sæ¶æ„ï¼ˆæµè§ˆå™¨/æœåŠ¡å™¨ï¼‰ | åªè¦ç”¨æˆ·å®‰è£…æµè§ˆå™¨ï¼ˆè°·æ­Œï¼‰å°±å¯ä»¥è®¿é—®   |
| C/Sæ¶æ„ï¼ˆå®¢æˆ·ç«¯/æœåŠ¡å™¨ï¼‰ | éœ€è¦ç”¨æˆ·å®‰è£…å’Œæ›´æ–°å®¢æˆ·ç«¯éƒ¨åˆ†ï¼Œæ¯”å¦‚å¾®ä¿¡ |

##### 5.1.2 å¼€å‘æ¨¡å¼åŠç›¸å…³æŠ€æœ¯

- å¼€å‘æ¨¡å¼ï¼š`MVC` æ¨¡å¼

| ç»„ä»¶       | è¯´æ˜                                   |
| ---------- | -------------------------------------- |
| Model      | æ¨¡å‹                                   |
| View       | è§†å›¾ï¼Œ`login.html` / `index.html`      |
| Controller | æ§åˆ¶å™¨ï¼Œæ¥å—ç”¨æˆ·çš„è¯·æ±‚ï¼Œå‘ç”¨æˆ·åšå‡ºå“åº” |

- å¼€å‘æ–¹å¼

| æ–¹å¼         | è¯´æ˜                       |
| ------------ | -------------------------- |
| å‰åç«¯ä¸åˆ†ç¦» | `JSP`/`HTML` + `SSM`       |
| å‰åç«¯åˆ†ç¦»   | å‰ç«¯å’Œåå°ä¸åœ¨åŒä¸€ä¸ªé¡¹ç›®ä¸­ |

- è¯·æ±‚æ–¹æ³•

| è¯·æ±‚æ–¹æ³•  | è¯´æ˜                 | å¸¸ç”¨æ–¹å¼                                                     |
| --------- | -------------------- | ------------------------------------------------------------ |
| **`GET`** | å¸¸ç”¨äºæŸ¥è¯¢ã€åˆ é™¤æ“ä½œ | 1. æµè§ˆå™¨çš„åœ°å€æ <br />2. `<a href="..."></a>`<br />3. `Ajax`<br />4. å¼‚æ­¥è¯·æ±‚ |
| `POST`    | å¸¸ç”¨äºå¢åŠ ã€ä¿®æ”¹æ“ä½œ | 1.  `Form`è¡¨å•<br />2. `Ajax`                                |

- `JavaWeb`å¼€å‘ç›¸å…³æŠ€æœ¯
  - Javaåå°æŠ€æœ¯
    - `SSM`ï¼šå¤§é‡çš„é…ç½®æ–‡ä»¶ `xml`ï¼Œ `SpringBoot`+ `MyBatis`
    - `Servlet`/`JSP`ã€`Spring` + `Spring MVC`ã€`SpringBoot`ã€`SpringCloud`ï¼ˆå¾®æœåŠ¡ï¼‰ 
    - æ•°æ®åº“ç›¸å…³æ¡†æ¶ï¼š`MyBatis`ã€`Hibernate`ã€`Spring JPA`

  - å‰ç«¯æŠ€æœ¯
    - `HTML/HTML5`ã€`CSS/CSS3`ã€`JS`
    - å‰ç«¯æ¡†æ¶ï¼š`jQuery`ã€`Vue`ã€`AngularJS`ã€`React`ã€`TS`


#### 5.2 `SpringBoot` + `MyBatis` æ¡†æ¶ç®€ä»‹

[Spring Bootï¼šå…¥é—¨ç¯‡ (cnblogs.com)](https://www.cnblogs.com/ityouknow/p/5662753.html)

[MyBatisä¸­æ–‡ç½‘](https://mybatis.net.cn/)

#### 5.3 åå°æœåŠ¡å¼€å‘

åŸºæœ¬ä»»åŠ¡è¯´æ˜ï¼š

- ç»Ÿè®¡ä»Šå¤©æ‰€æœ‰æ™¯ç‚¹çš„äººæµé‡æ€»å’Œ
- ç»Ÿè®¡å½“å‰æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰æ‰€æœ‰æ™¯ç‚¹çš„äººæµé‡æ€»å’Œ
- ç»Ÿè®¡æ¯åˆ†é’Ÿæ¯ä¸ªæ™¯ç‚¹çš„äººæµé‡ï¼ˆç»çº¬åº¦ï¼‰-çƒ­åŠ›å›¾

##### 5.3.1 é¡¹ç›®ç¯å¢ƒæ­å»º

1. åˆ›å»º`SpringBoot`é¡¹ç›®

<img src="./OnlineTravelBigdataPlatform/NewModule.png" style="zoom: 50%;" >

<img src="./OnlineTravelBigdataPlatform/NewModule2.png" style="zoom: 50%;" >

<img src="./OnlineTravelBigdataPlatform/NewModule3.png" style="zoom: 50%;" >

<img src="./OnlineTravelBigdataPlatform/NewModule4.png" style="zoom: 50%;" >

2. ä¿®æ”¹å¿…è¦é…ç½®

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <!-- è¿™é‡Œéœ€è¦ä¿®æ”¹ç‰ˆæœ¬ -->
        <version>2.7.15</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>com.example.x.travel</groupId>
    <artifactId>travelweb</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>travelweb</name>
    <description>Travel Project Web</description>
    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>

        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.41</version>
        </dependency>
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>druid-spring-boot-starter</artifactId>
            <version>1.2.18</version>
        </dependency>
        <dependency>
            <groupId>org.mybatis.spring.boot</groupId>
            <artifactId>mybatis-spring-boot-starter</artifactId>
            <version>1.3.2</version>
        </dependency>
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-lang3</artifactId>
        </dependency>
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>1.2.17</version>
        </dependency>

    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
                <configuration>
                    <excludes>
                        <exclude>
                            <groupId>org.projectlombok</groupId>
                            <artifactId>lombok</artifactId>
                        </exclude>
                    </excludes>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
```

3. ä¿®æ”¹ `application.xml` ä¸º `application.yml` å¹¶è¿›è¡Œé…ç½®

```yaml
# é…ç½®åº”ç”¨æœåŠ¡å™¨ç«¯å£ä»¥åŠæ ¹ç›®å½•
server:
  port: 9090
  servlet:
    context-path: /travelserver
# é…ç½®åº”ç”¨å
spring:
  application:
    name: travelserver
  # é…ç½®æ•°æ®æº, è¿™é‡Œæ˜¯ Druid çš„ä¸€äº›é…ç½®
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    druid:
      url: jdbc:mysql://bigdata:3306/travel?useSSL=false&characterEncoding=UTF-8
      username: root
      password: root
      driver-class-name: com.mysql.jdbc.Driver
      initial-size: 1
      minIdle: 1
      maxActive: 5
      maxWait: 6000
      timeBetweenEvictionRunsMillis: 6000
      minEvictableIdleTimeMillis: 30000
      validationQuery: SELECT 1 FROM DUAL
      testWhileIdle: true
      testOnBorrow: false
      testOnReturn: false
      filters: stat,wall,log4j
# é…ç½® MyBatis çš„è·¯å¾„
mybatis:
  # è¿™é‡Œæ˜¯ DAO Mapperçš„è·¯å¾„, è¡¨ç¤º Resource/Mapper/ çš„æ‰€æœ‰ *.xml æ–‡ä»¶
  mapper-locations: classpath:mapper/*.xml
  # è¿™é‡Œå®šä¹‰å®ä½“ç±»çš„ä½ç½®
  type-aliases-package: com.example.x.travel.travelweb.entity
  configuration:
    map-underscore-to-camel-case: true
```

4. åœ¨ **IDEA** ä¸­æ·»åŠ  `Lombok` å’Œ `MyBatisX` çš„æ’ä»¶
5. é…ç½® `MyBatis` çš„ `*.xml` æ¨¡æ¿

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
      "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper> <!-- è¿™é‡Œè¡¥å…¨å†…å®¹ -->

</mapper>
```

6. å¼€å‘ `TravelConfig.java` ç±»è§£å†³è·¨åŸŸé—®é¢˜

```java
// SpringBoot ä¸­å¾ˆå¤šäº‹æƒ…éœ€è¦é€šè¿‡æ³¨è§£çš„æ–¹å¼æ¥å®ç°
// @... ä»£è¡¨æ³¨è§£
// @Configuration æ³¨è§£è¡¨æ˜è¿™æ˜¯ä¸€ä¸ª Spring Boot çš„é…ç½®ç±»
// @EnableWebMvc  å¼€å¯ WebMvc çš„åŠŸèƒ½, è¿™æ · Spring Boot å°±èƒ½è‡ªåŠ¨å¤„ç† Web ç›¸å…³çš„é…ç½®
@Configuration
@EnableWebMvc
public class TravelConfig implements WebMvcConfigurer {
    /**
     * @brief è®¾ç½®æœåŠ¡å™¨è·¨åŸŸè®¿é—®ç­–ç•¥: æ‰€æœ‰ GET å’Œ POST è¯·æ±‚éƒ½å¯ä»¥è·¨åŸŸè®¿é—®, å…è®¸æ‰€æœ‰æ¥æºçš„è¯·æ±‚, å³å…è®¸è·¨åŸŸè¯·æ±‚
     *        å› ä¸ºæˆ‘ä»¬çš„é¡¹ç›®æ˜¯å‰åç«¯åˆ†ç¦»çš„, å³å‰ç«¯å’Œåç«¯ä¸åœ¨ä¸€ä¸ªæœåŠ¡å™¨ä¸Š, æ‰€ä»¥éœ€è¦å¤„ç†è·¨åŸŸé—®é¢˜
     * @param registry
     */
    @Override
    public void addCorsMappings(CorsRegistry registry) {
        registry.addMapping("/**")  // addMapping æ–¹æ³•çš„å‚æ•°ä»£è¡¨éœ€è¦è¿›è¡Œ CORS æ˜ å°„çš„ HTTPæ–¹æ³•åŠå…¶è·¯å¾„, è¿™é‡Œæ˜¯"/**", æ„å‘³ç€å¯¹æ‰€æœ‰HTTPæ–¹æ³•è¿›è¡Œæ˜ å°„
                .allowedOriginPatterns("*")     // allowedOriginPatterns æ˜¯ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨, è¿™é‡Œä½¿ç”¨ "*" è¡¨ç¤ºä»»ä½•æ¥æºéƒ½å¯ä»¥
                .allowedMethods("GET", "POST")  // allowedMethodsæ˜¯å…è®¸çš„ HTTP æ–¹æ³•ï¼Œè¿™é‡Œæ˜¯ "GET,POST"
                .allowCredentials(true)         // å…è®¸æºå¸¦è¯·æ±‚ä½“çš„è¯·æ±‚é€šè¿‡, è¿™é‡Œçš„ allowCredentials(true) æ„å‘³ç€åœ¨è¯·æ±‚å¤´ä¸­æºå¸¦ç”¨æˆ·å‡­è¯ä¿¡æ¯æ˜¯å…è®¸çš„
                .maxAge(3600);                  // maxAge è®¾ç½® CORS è·¨åŸŸé…ç½®çš„ç¼“å­˜æ—¶é—´, è¿™é‡Œæ˜¯ 3600 ç§’
    }
}
```

7. æœ¬é¡¹ç›®ï¼ˆ`Java`å¼€å‘ï¼‰ä¸­å±‚ä¸å±‚ä¹‹é—´çš„è”ç³»

<img src="./OnlineTravelBigdataPlatform/LayersToExecuteTravelWeb.png" style="zoom: 33%;" />

##### 5.3.2 åå°å¼€å‘

- `com.example.x.travel.travelweb.config.TravelConfig`

```java
import org.springframework.context.annotation.Configuration;
import org.springframework.web.servlet.config.annotation.CorsRegistry;
import org.springframework.web.servlet.config.annotation.EnableWebMvc;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;

// SpringBoot ä¸­å¾ˆå¤šäº‹æƒ…éœ€è¦é€šè¿‡æ³¨è§£çš„æ–¹å¼æ¥å®ç°
// @... ä»£è¡¨æ³¨è§£
// @Configuration æ³¨è§£è¡¨æ˜è¿™æ˜¯ä¸€ä¸ª Spring Boot çš„é…ç½®ç±»
// @EnableWebMvc  å¼€å¯ WebMvc çš„åŠŸèƒ½, è¿™æ · Spring Boot å°±èƒ½è‡ªåŠ¨å¤„ç† Web ç›¸å…³çš„é…ç½®
@Configuration
@EnableWebMvc
public class TravelConfig implements WebMvcConfigurer {
    /**
     * @brief è®¾ç½®æœåŠ¡å™¨è·¨åŸŸè®¿é—®ç­–ç•¥: æ‰€æœ‰ GET å’Œ POST è¯·æ±‚éƒ½å¯ä»¥è·¨åŸŸè®¿é—®, å…è®¸æ‰€æœ‰æ¥æºçš„è¯·æ±‚, å³å…è®¸è·¨åŸŸè¯·æ±‚
     *        å› ä¸ºæˆ‘ä»¬çš„é¡¹ç›®æ˜¯å‰åç«¯åˆ†ç¦»çš„, å³å‰ç«¯å’Œåç«¯ä¸åœ¨ä¸€ä¸ªæœåŠ¡å™¨ä¸Š, æ‰€ä»¥éœ€è¦å¤„ç†è·¨åŸŸé—®é¢˜
     * @param registry
     */
    @Override
    public void addCorsMappings(CorsRegistry registry) {
        registry.addMapping("/**")  // addMapping æ–¹æ³•çš„å‚æ•°ä»£è¡¨éœ€è¦è¿›è¡Œ CORS æ˜ å°„çš„ HTTPæ–¹æ³•åŠå…¶è·¯å¾„, è¿™é‡Œæ˜¯"/**", æ„å‘³ç€å¯¹æ‰€æœ‰HTTPæ–¹æ³•è¿›è¡Œæ˜ å°„
                .allowedOriginPatterns("*")     // allowedOriginPatterns æ˜¯ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨, è¿™é‡Œä½¿ç”¨ "*" è¡¨ç¤ºä»»ä½•æ¥æºéƒ½å¯ä»¥
                .allowedMethods("GET", "POST")  // allowedMethodsæ˜¯å…è®¸çš„ HTTP æ–¹æ³•ï¼Œè¿™é‡Œæ˜¯ "GET,POST"
                .allowCredentials(true)         // å…è®¸æºå¸¦è¯·æ±‚ä½“çš„è¯·æ±‚é€šè¿‡, è¿™é‡Œçš„ allowCredentials(true) æ„å‘³ç€åœ¨è¯·æ±‚å¤´ä¸­æºå¸¦ç”¨æˆ·å‡­è¯ä¿¡æ¯æ˜¯å…è®¸çš„
                .maxAge(3600);                  // maxAge è®¾ç½® CORS è·¨åŸŸé…ç½®çš„ç¼“å­˜æ—¶é—´, è¿™é‡Œæ˜¯ 3600 ç§’
    }
}
```

- `com.example.x.travel.travelweb.controller.TravelController`

```java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

@RestController                 // æ³¨è§£: å®šä¹‰è¿™ä¸ªç±»æ˜¯ä¸€ä¸ª Controller
@RequestMapping("/scenic")      // æ³¨è§£: å®šä¹‰è®¿é—®è¿™ä¸ªç±»çš„ URL
// è®¿é—® URL: http://ip:port/scenic
public class TravelController {
    @Autowired
    private TravelService travelService;

    // @RequestMapping è¡¨ç¤ºæ—¢å¯ä»¥é€šè¿‡ GET è®¿é—®, ä¹Ÿå¯ä»¥é€šè¿‡ POST è®¿é—®
    // @GetMapping     è¡¨ç¤ºåªå¯ä»¥é€šè¿‡ GET è®¿é—®
    // @PostMapping    è¡¨ç¤ºåªå¯ä»¥é€šè¿‡ POST è®¿é—®
    /**
     * @brief   è¡¨ç¤ºè·å–ä¸€å¤©è®¿å®¢çš„æ€»æ•°é‡
     * @url     http://localhost:9090/travelserver/scenic/getDaySum
     * @return
     */
    @RequestMapping("/getDaySum")
    public long getDaySum() {
        long sum = travelService.getDaySum();
        return sum;
    }

    @RequestMapping("/getCurrentSum")
    public long getCurrentSum() {
        long sum = travelService.getCurrentSum();
        return sum;
    }

    @RequestMapping("/getHeatData")
    public List<Scenic> getHeatData() {
        return travelService.getHeatData();
    }

    @RequestMapping("/getScenicMinuteData")
    public List<Scenic> getScenicMinuteData() {
        return travelService.getScenicMinuteData();
    }

    @RequestMapping("/getScenicTrend")
    public HashMap<String, Object> getScenicTrend() {
        return travelService.getScenicTread();
    }

    @PostMapping("/selectForm")
    @ResponseBody
    public Object receiveFormData(@RequestBody Map<String, String> formData) {
        // åœ¨è¿™é‡Œå¯ä»¥å¤„ç†æ”¶åˆ°çš„formData
        // System.out.println(formData);
        String selectDate = formData.get("date");
        String selectFromTime = formData.get("fromTime");
        String selectToTime = formData.get("toTime");
        String selectTableName = formData.get("tableName");
        // System.out.println(selectDate + " " + selectFromTime + " " + selectToTime + " " + selectTableName);

        // æ‰§è¡Œä¸šåŠ¡é€»è¾‘
        if (selectDate.equals("")) {
            return null;
        }
        String fromDateAndTime = selectDate.replace("-", "") + selectFromTime.replace(":", "").replace("ï¼š", "");
        String toDateAndTime = selectDate.replace("-", "") + selectToTime.replace(":", "").replace("ï¼š", "");
        System.out.println("From Date And Time: " + fromDateAndTime + "\nTo Date And Time: " + toDateAndTime);
        switch (selectTableName) {
            case "0": {
                System.out.println("è¯·è¿›è¡Œé€‰æ‹©");
                break;
            }
            case "1": { // çƒ­åŠ›å›¾
                return travelService.getHeatDataByDateAndTime(fromDateAndTime, toDateAndTime);
            }
            case "2": { // äººæµé‡æŸ±çŠ¶å›¾
                return travelService.getScenicDataByDateAndTime(fromDateAndTime, toDateAndTime);
            }
            case "3": { // äººæµé‡è¶‹åŠ¿å›¾
                return travelService.getScenicTreadByDateAndTime(fromDateAndTime, toDateAndTime);
            }
            default:    // æœªå®šä¹‰å›¾è¡¨
                break;
        }
        return "Data received successfully";
    }
}
```

- `com.example.x.travel.travelweb.dao.TravelDao`

```java
import org.springframework.stereotype.Repository;

import java.util.HashMap;
import java.util.List;

@Repository
public interface TravelDao {
    long getDaySum();           // æŸ¥è¯¢å½“å¤©æ—¶é—´å†…çš„æ‰€æœ‰æ•°æ®ä¹‹å’Œ
    long getCurrentSum();       // æŸ¥è¯¢å½“å‰æ—¶é—´å†…çš„æ‰€æœ‰æ•°æ®ä¹‹å’Œ
    List<Scenic> getHeatData();         // æŸ¥è¯¢ç”¨äºåˆ›å»ºçƒ­åŠ›å›¾çš„æ•°æ®
    List<Scenic> getScenicMinuteData(); // æŸ¥è¯¢ç”¨äºè·å–æ¯åˆ†é’Ÿæ™¯ç‚¹äººæµé‡
    List<Scenic> getScenicTread();      // æŸ¥è¯¢ç”¨äºè·å–æ¯åˆ†é’Ÿæ™¯ç‚¹äººæµé‡è¶‹åŠ¿

    List<Scenic> getHeatDataByDateAndTime(String fromDateAndTime, String toDateAndTime);
    List<Scenic> getScenicDataByDateAndTime(String fromDateAndTime, String toDateAndTime);
    List<Scenic> getScenicTreadByDateAndTime(String fromDateAndTime, String toDateAndTime);
}

```

- `com.example.x.travel.travelweb.entity.LineData`

```java
import lombok.Data;

import java.util.List;

@Data
public class LineData {
    private String      name;
    private List<Long>  data;
}
```

- `com.example.x.travel.travelweb.entity.Scenic`

```java
import lombok.Data;

@Data
public class Scenic {
    private double lng;
    private double lat;
    private long count;
    private String time;
    private String scenic;
}
```

- `com.example.x.travel.travelweb.services.TravelService`

```java
package com.example.x.travel.travelweb.services;

import com.example.x.travel.travelweb.entity.Scenic;

import java.util.HashMap;
import java.util.List;

// æ¥å£: å®šä¹‰è§„èŒƒçš„
public interface TravelService {
    long getDaySum();
    long getCurrentSum();
    List<Scenic> getHeatData();
    List<Scenic> getScenicMinuteData();
    HashMap<String, Object> getScenicTread();

    List<Scenic> getHeatDataByDateAndTime(String fromDateAndTime, String toDateAndTime);
    List<Scenic> getScenicDataByDateAndTime(String fromDateAndTime, String toDateAndTime);
    HashMap<String, Object> getScenicTreadByDateAndTime(String fromDateAndTime, String toDateAndTime);
}

```

- `com.example.x.travel.travelweb.services.impl.TravelServiceImpl`

```java
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;

@Service
public class TravelServiceImpl implements TravelService {
    // è°ƒç”¨æŸä¸ªå¯¹è±¡çš„æ–¹æ³•, éœ€è¦å…ˆåˆ›å»ºå¯¹è±¡
    @Autowired
    private TravelDao travelDao;    // ç›¸å½“äº TravelDap travelDao = new TravelDao();

    @Override
    public long getDaySum() {
        return travelDao.getDaySum() ;
    }

    @Override
    public long getCurrentSum() {
        return travelDao.getCurrentSum();
    }

    @Override
    public List<Scenic> getHeatData() {
        return travelDao.getHeatData();
    }

    @Override
    public List<Scenic> getScenicMinuteData() {
        return travelDao.getScenicMinuteData();
    }

    @Override
    public HashMap<String, Object> getScenicTread() {
        HashMap<String, Object> maps = new HashMap<>();
        List<String> times = new ArrayList<>();     // æ—¶é—´æ•°ç»„
        List<LineData> series = new ArrayList<>();  // Series æ•°ç»„

        List<Scenic> heat = travelDao.getScenicTread();     // è°ƒç”¨ DAO æ–¹æ³•æŸ¥è¯¢æ•°æ®
        HashMap<String, List<Long>> map = new HashMap<>();  // å°†æ™¯ç‚¹ä»¥åŠè¿™ä¸ªæ™¯ç‚¹å¯¹åº”çš„æ‰€æœ‰æ•°æ®æ•´åˆæˆä¸€æ¡æ•°æ®
        for(Scenic scenic: heat) {
            String time = scenic.getTime();                 // è·å–æ•°æ®åº“æ¯æ¡æ•°æ®çš„æ—¶é—´
            if (!times.contains(time)) {                    // è¿™ä¸ªæ—¶é—´åœ¨ times ä¸­æ˜¯å¦å­˜åœ¨
                times.add(time);                            // ä¸å­˜åœ¨åˆ™æ·»åŠ 
            }
            String scenic_name = scenic.getScenic();        // è·å–æ™¯ç‚¹åç§°
            // åˆ¤æ–­åœ¨ Map ä¸­æ˜¯å¦æœ‰ key, å¹¶åˆ¤æ–­:
            // å¦‚æœæœ‰è¿™ä¸ª key, åˆ™æŠŠæ•°æ®æ·»åŠ åˆ° value ä¸­
            // å¦‚æœæ²¡æœ‰è¿™ä¸ª key, åˆ™æ·»åŠ è¿™ä¸ª key, æ–°æ·»åŠ ä¸€ä¸ªé›†åˆ
            map.computeIfAbsent(scenic_name, k->new ArrayList<>()).add(scenic.getCount());

            // ä¸Šé¢ä¸€è¡Œç­‰åŒäºä¸‹é¢çš„é€»è¾‘
            // if (map.containsKey(scienc)){
            //     map.get(scienc).add(scenic.getCount());
            // } else {
            //     List list=new ArrayList<Long>();
            //     list.add(scenic.getCount());
            //     map.put(scienc,list);
            // }
        }

        // System.out.println(times);
        // System.out.println(map);

        map.forEach((key, value) -> {
            LineData data = new LineData();
            data.setName(key);
            data.setData(value);
            series.add(data);
        });

        maps.put("time",    times);
        maps.put("series", series);
        return maps;
    }

    @Override
    public List<Scenic> getHeatDataByDateAndTime(String fromDateAndTime, String toDateAndTime) {
        return travelDao.getHeatDataByDateAndTime(fromDateAndTime, toDateAndTime);
    }

    @Override
    public List<Scenic> getScenicDataByDateAndTime(String fromDateAndTime, String toDateAndTime) {
        return travelDao.getScenicDataByDateAndTime(fromDateAndTime, toDateAndTime);
    }

    @Override
    public HashMap<String, Object> getScenicTreadByDateAndTime(String fromDateAndTime, String toDateAndTime) {
        return null;
    }

}

```

- `mapper/TravelMapper.xml`

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN"
        "http://mybatis.org/dtd/mybatis-3-mapper.dtd">
<mapper namespace="com.inspur.szy.subowen.travel.travelweb.dao.TravelDao">


    <select id="getDaySum" resultType="java.lang.Long">
        SELECT IFNULL(SUM(min_quantity), 0) AS cnt FROM t_scenic WHERE SUBSTRING(`min_moment`, 1, 8)=DATE_FORMAT(NOW(), "%Y%m%d")
    </select>
    <select id="getCurrentSum" resultType="java.lang.Long">
        SELECT IFNULL(SUM(min_quantity), 0) AS cnt FROM t_scenic WHERE SUBSTRING(`min_moment`, 1, 12)=DATE_FORMAT(NOW(), "%Y%m%d%H%i")
    </select>
    <select id="getHeatData" resultType="com.inspur.szy.subowen.travel.travelweb.entity.Scenic">
        SELECT longitude AS `lng`, latitude AS `lat`, SUBSTRING(`sec_moment`, 1, 12) AS today, SUM(`sec_quantity`) AS `count`
        FROM t_heat GROUP BY lng, lat, today HAVING today=DATE_FORMAT(NOW(), '%Y%m%d%H%i')
    </select>
    <select id="getScenicMinuteData" resultType="com.inspur.szy.subowen.travel.travelweb.entity.Scenic">
        SELECT scenic, min_moment AS `time`, min_quantity AS `count`
        FROM t_scenic
        WHERE min_moment = (SELECT MAX(min_moment) FROM t_scenic WHERE <![CDATA[min_moment <= NOW()]]>)
        GROUP BY scenic, min_moment;
    </select>
    <select id="getScenicTread" resultType="com.inspur.szy.subowen.travel.travelweb.entity.Scenic">
        SELECT scenic, min_moment AS `time`, min_quantity AS `count`
        FROM t_scenic
        WHERE SUBSTRING(`min_moment`, 1, 8)=DATE_FORMAT(NOW(), '%Y%m%d') ORDER BY `time`
    </select>
    <select id="getHeatDataByDateAndTime" resultType="com.inspur.szy.subowen.travel.travelweb.entity.Scenic">
        SELECT longitude AS `lng`, latitude AS `lat`, SUBSTRING(`sec_moment`, 1, 12) AS day, SUM(`sec_quantity`) AS `count`
        FROM t_heat GROUP BY lng, lat, day HAVING <![CDATA[day >= #{fromDateAndTime} AND day <= #{toDateAndTime}]]>
    </select>
    <select id="getScenicDataByDateAndTime" resultType="com.inspur.szy.subowen.travel.travelweb.entity.Scenic">
        SELECT scenic, SUBSTRING(`min_moment`, 1, 12) AS `time`, min_quantity AS `count`
        FROM t_scenic
        WHERE <![CDATA[min_moment >= #{fromDateAndTime} AND min_moment <= #{toDateAndTime}]]>
        GROUP BY `scenic`, `min_moment`;
    </select>
    <select id="getScenicTreadByDateAndTime" resultType="com.inspur.szy.subowen.travel.travelweb.entity.Scenic">

    </select>
</mapper>
```

#### 5.4 å‰ç«¯å¼€å‘

æ‰€æœ‰ä»£ç åœ¨ `GitHub` å‡æœ‰å¼€æºï¼Œæˆ‘åšçš„ä¹Ÿå°±åªæ˜¯æ·»åŠ  `Ajax` è¯·æ±‚è€Œå·²ã€‚

### â“ é—®é¢˜æ±‡æ€»

| åºå· | å‘ç”Ÿæ—¥æœŸ   | é—®é¢˜æè¿°                                                     | æ˜¯å¦è§£å†³ | è§£å†³æªæ–½                                                     | å¤‡æ³¨         |
| ---- | ---------- | ------------------------------------------------------------ | -------- | ------------------------------------------------------------ | ------------ |
| 0001 | 2024-06-13 | æ‰§è¡Œä»£ç ï¼š<br><img src="https://img-blog.csdnimg.cn/direct/2e01c35c37f143edaeef11a431df32d7.png#pic_center" alt="Qusetion0001" style="zoom: 50%;" /><br>å‡ºç°ç¼–è¯‘é”™è¯¯ï¼š<br><img src="https://img-blog.csdnimg.cn/direct/499491d731204bc0a433dd4ab71fd5c2.png" alt="Qusetion0001" style="zoom: 50%;" /> | æ˜¯       | ä¿®æ”¹ä»£ç ï¼š<br>`val topics = "..."` ä¸º `val topics = Array("...")`<br>è¿™é‡ŒæŠ¥é”™çš„åŸå› æ˜¯å‡ºç°äº†æ— æ³•é‡è½½æ–¹æ³•çš„é—®é¢˜ã€‚å› ä¸ºé”™è¯¯çš„ä¼ é€’äº†æ‰€éœ€è¦çš„å‚æ•°ï¼Œå¯¼è‡´`Subscribe`æ–¹æ³•å‡ºç°äº†ä¸æœŸå¾…çš„é‡è½½ã€‚ä¿®æ”¹ä»£ç åˆ°æ­£ç¡®çš„æ ¼å¼å³å¯ã€‚ | è€å¸ˆååŠ©è§£å†³ |
| 0002 | 2024-06-13 | ä½¿ç”¨`updateStatusByKey`å‡ºç°æ•°æ®åº“æ•°æ®ä¸æ–­å¢åŠ çš„é—®é¢˜<br>***é—®é¢˜åŸå› ***ï¼š<br>1. MySQLæ•°æ®è¡¨è®¾è®¡ä¸åˆç†ï¼Œè®¾ç½®äº†ä¸€ä¸ªåˆ—ä¸º `id` å¹¶å°†å…¶è®¾ç½®ä¸ºä¸»é”®<br>2. SQLè¯­å¥æœ‰å¾…ä¼˜åŒ– | æ˜¯       | 1. ä¿®æ”¹MySQLè¡¨çš„ç»“æ„ï¼Œè®¾ç½®è”åˆä¸»é”® <br>2. ä¿®æ”¹SQLè¯­å¥ï¼Œä½¿ç”¨ `ON DUPLICATE KEY` è¯­æ³• | è€å¸ˆååŠ©è§£å†³ |
| 0003 | 2024-06-13 | æ‰§è¡Œç¨‹åºå‡ºç°é”™è¯¯ï¼š<br>`ERROR [main] (Logging.scala:94) - Failed to bind SparkUI`<br>***é—®é¢˜åŸå› ***ï¼š<br>æ¯ä¸€ä¸ª`Spark`ä»»åŠ¡éƒ½ä¼šå ç”¨ä¸€ä¸ª`SparkUI`ç«¯å£ï¼Œé»˜è®¤ä¸º`4040`ï¼Œå¦‚æœè¢«å ç”¨åˆ™ä¾æ¬¡é€’å¢ç«¯å£é‡è¯•ã€‚ä½†æ˜¯æœ‰ä¸ªé»˜è®¤é‡è¯•æ¬¡æ•°ï¼Œä¸º`16`æ¬¡ã€‚`16`æ¬¡é‡è¯•éƒ½å¤±è´¥åï¼Œä¼šæ”¾å¼ƒè¯¥ä»»åŠ¡çš„è¿è¡Œã€‚ | æ˜¯       | åˆå§‹åŒ– `SparkConf` æ—¶ï¼Œæ·»åŠ `conf.set("spark.port.maxRetries", "100")`è¯­å¥ï¼›ä½¿ç”¨ `spark-submit`æäº¤ä»»åŠ¡æ—¶ï¼Œåœ¨å¯åŠ¨å‘½ä»¤è¡Œä¸­æ·»åŠ `â€“conf spark.port.maxRetries=100 \`<br>è¯¥å‚æ•°è®¾ç½®å‘åé€’å¢`100`æ¬¡å¯»æ‰¾ç«¯å£ | è‡ªä¸»è§£å†³     |
| 0004 | 2024-06-17 | ä¸æ˜¯å¾ˆä¸¥é‡çš„é—®é¢˜ï¼Œå½“`return 0` æˆ–æœªäº§ç”Ÿæ•°æ®æ—¶ï¼Œä½¿ç”¨ **Edge** æµè§ˆå™¨ä¼šå‡ºç°å±å¹•ä¸Šæ²¡æœ‰è¾“å‡ºå€¼çš„æƒ…å†µ | æ˜¯       | æ›¿æ¢ä¸º**Chrome**å³å¯è§£å†³é—®é¢˜ï¼Œä¸è¿‡ä¹Ÿæ²¡å•¥å¿…è¦                 | æ°´           |
| 0005 | 2024-06-18 | åœ¨ `TravelMapper.xml` ä¸­ç¼–å†™å¸¦ `<` å’Œ`>` çš„ `SQL` ä»£ç ï¼Œå¯¼è‡´å‡ºç°é—®é¢˜ï¼š<br />`Caused by: org.xml.sax.SAXParseException: å…ƒç´ å†…å®¹å¿…é¡»ç”±æ ¼å¼æ­£ç¡®çš„å­—ç¬¦æ•°æ®æˆ–æ ‡è®°ç»„æˆã€‚`<br />**é—®é¢˜åŸå› **ï¼š<br />XMLæ–‡ä»¶ä¼šå°†`<` æˆ– `>` å½“ä½œæ˜¯æ ‡è®°ï¼Œå¯¼è‡´é”™è¯¯ | æ˜¯       | [å‚è€ƒæ–‡ç« ](https://blog.csdn.net/miantian180/article/details/82255910)<br />ä½¿ç”¨æ ‡è®°`<![CDATA[ ]]>`å°†åŒ…å« `<` æˆ– `>` çš„è¯­å¥åŒ…å«ä½ï¼Œå¦‚ï¼š`<![CDATA[min_moment <= NOW()]]>` | è‡ªä¸»è§£å†³     |
| 0006 | 2024-06-19 | åŒå­¦å‡ºç°é—®é¢˜ï¼š<br />ç”Ÿäº§è€…æ•°æ®æ­£å¸¸å†™å…¥ã€å®¢æˆ·ç«¯ï¼ˆWindowsï¼‰å’ŒæœåŠ¡å™¨ç«¯ï¼ˆLinuxï¼‰æ—¶é—´æ­£ç¡®ï¼ŒMySQLæ— æ³•é€šè¿‡`NOW()` è·å–åˆ°å½“å‰æ—¶é—´çš„æ•°æ®ã€‚ | æ˜¯       | MySQLæ—¶åŒºé—®é¢˜ã€‚                                              | ååŠ©åŒå­¦è§£å†³ |
| 0007 | 2024-06-20 | åŒå­¦å‡ºç°é—®é¢˜ï¼š<br />è™šæ‹Ÿæœºæ‰ç½‘å¡                             | æ˜¯       | [è™šæ‹Ÿæœºç½‘å¡ä¸è§äº†,é‡æ–°å¼€æœºè™šæ‹Ÿæœºç½‘å¡æ¶ˆå¤±è¿æ¥ä¸ä¸Š--è™šæ‹Ÿæœºç½‘å¡æ‰äº†](https://blog.csdn.net/ganchangshao/article/details/82992503) | ååŠ©åŒå­¦è§£å†³ |





